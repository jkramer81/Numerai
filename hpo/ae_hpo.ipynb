{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import datetime\n",
        "import os\n",
        "import copy\n",
        "import joblib\n",
        "import neptune\n",
        "import neptunecontrib.monitoring.optuna as opt_utils\n",
        "import optuna\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn as nn\n",
        "from optuna.integration import PyTorchLightningPruningCallback\n",
        "from pytorch_lightning import Callback\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "from torch.utils.data import Subset, BatchSampler, SequentialSampler, DataLoader\n",
        "import torch\n",
        "import numpy as np\n",
        "from models.SupervisedAutoEncoder import SupAE\n",
        "from data_loading.purged_group_time_series import PurgedGroupTimeSeriesSplit\n",
        "from data_loading.utils import load_data, preprocess_data, FinData, weighted_mean, seed_everything, calc_data_mean, \\\n",
        "    create_dataloaders\n",
        "\n",
        "\n",
        "class MetricsCallback(Callback):\n",
        "    \"\"\"PyTorch Lightning metric callback.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.metrics = []\n",
        "\n",
        "    def on_validation_end(self, trainer, pl_module):\n",
        "        self.metrics.append(trainer.callback_metrics)\n",
        "\n",
        "\n",
        "def create_param_dict(trial, trial_file=None):\n",
        "    if trial and not trial_file:\n",
        "        dim_1 = trial.suggest_int('dim_1', 500, 1000)\n",
        "        dim_2 = trial.suggest_int('dim_2', 250, 500)\n",
        "        dim_3 = trial.suggest_int('dim_3', 100, 250)\n",
        "        hidden = trial.suggest_int('hidden', 50, 200)\n",
        "        act_func = trial.suggest_categorical(\n",
        "            'activation', ['relu', 'leaky_relu', 'gelu', 'silu'])\n",
        "        act_dict = {'relu': nn.ReLU, 'leaky_relu': nn.LeakyReLU,\n",
        "                    'gelu': nn.GELU, 'silu': nn.SiLU}\n",
        "        act_func = act_dict[act_func]\n",
        "        dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n",
        "        lr = trial.suggest_uniform('lr', 0.00005, 0.05)\n",
        "        recon_loss_factor = trial.suggest_uniform('recon_loss_factor', 0.1, 1)\n",
        "        p = {'dim_1': dim_1, 'dim_2': dim_2, 'dim_3': dim_3, 'hidden': hidden,\n",
        "             'activation': act_func, 'dropout': dropout,\n",
        "             'lr': lr, 'recon_loss_factor': recon_loss_factor, 'loss_sup_ae': nn.MSELoss,\n",
        "             'loss_recon': nn.MSELoss,\n",
        "             'embedding': True}\n",
        "    elif trial and trial_file:\n",
        "        p = joblib.load(trial_file).best_params\n",
        "        if not p.get('dim_5', None):\n",
        "            p['dim_5'] = 75\n",
        "        if not p.get('label_smoothing', None):\n",
        "            p['label_smoothing'] = 0.094\n",
        "        act_dict = {'relu': nn.ReLU,\n",
        "                    'leaky_relu': nn.LeakyReLU, 'gelu': nn.GELU}\n",
        "        act_func = trial.suggest_categorical(\n",
        "            'activation', ['leaky_relu', 'gelu'])\n",
        "        p['activation'] = act_dict[p['activation']]\n",
        "    return p\n",
        "\n",
        "\n",
        "def optimize(trial: optuna.Trial, data_dict):\n",
        "    gts = PurgedGroupTimeSeriesSplit(n_splits=5, group_gap=5)\n",
        "    input_size = data_dict['data'].shape[-1]\n",
        "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "        os.path.join('hpo/checkpoints/', \"trial_ae_{}\".format(trial.number)), monitor=\"val_sup_loss\", mode='min')\n",
        "    logger = MetricsCallback()\n",
        "    metrics = []\n",
        "    sizes = []\n",
        "    # trial_file = 'HPO/nn_hpo_2021-01-05.pkl'\n",
        "    trial_file = None\n",
        "    p = create_param_dict(trial, trial_file)\n",
        "    p['batch_size'] = trial.suggest_int('batch_size', 500, 2000)\n",
        "    p['input_size'] = input_size\n",
        "    p['output_size'] = 1\n",
        "    print(f'Running Trail with params: {p}')\n",
        "    for i, (train_idx, val_idx) in enumerate(gts.split(data_dict['data'], groups=data_dict['era'])):\n",
        "        model = SupAE(params=p)\n",
        "        # model.apply(init_weights)\n",
        "        dataset = FinData(\n",
        "            data=data_dict['data'], target=data_dict['target'], era=data_dict['era'])\n",
        "        dataloaders = create_dataloaders(\n",
        "            dataset, indexes={'train': train_idx, 'val': val_idx}, batch_size=p['batch_size'])\n",
        "        es = EarlyStopping(monitor='val_loss', patience=10,\n",
        "                           min_delta=0.0005, mode='min')\n",
        "        trainer = pl.Trainer(logger=False,\n",
        "                             max_epochs=100,\n",
        "                             gpus=1,\n",
        "                             callbacks=[checkpoint_callback, logger, PyTorchLightningPruningCallback(\n",
        "                                 trial, monitor='val_sup_loss'), es],\n",
        "                             precision=16)\n",
        "        trainer.fit(\n",
        "            model, train_dataloader=dataloaders['train'], val_dataloaders=dataloaders['val'])\n",
        "        val_loss = logger.metrics[-1]['val_sup_loss'].item()\n",
        "        metrics.append(val_loss)\n",
        "        sizes.append(len(train_idx))\n",
        "    metrics_mean = weighted_mean(metrics, sizes)\n",
        "    return metrics_mean\n",
        "\n",
        "\n",
        "def main():\n",
        "    seed_everything(0)\n",
        "    data = load_data(root_dir='./data/', mode='train')\n",
        "    data, target, features, era = preprocess_data(\n",
        "        data, ordinal=True)\n",
        "    api_token = 'eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzMGQ1MWZiNy1iYjNlLTQ3NDctOTE4OS1lNzhlNmVlYmUwMzYifQ=='\n",
        "    neptune.init(api_token=api_token,\n",
        "                 project_qualified_name='kramerji/Numerai')\n",
        "    nn_exp = neptune.create_experiment('SupAE_HPO')\n",
        "    nn_neptune_callback = opt_utils.NeptuneCallback(experiment=nn_exp)\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    data_dict = {'data': data, 'target': target,\n",
        "                 'features': features, 'era': era}\n",
        "    study.optimize(lambda trial: optimize(trial, data_dict=data_dict), n_trials=100,\n",
        "                   callbacks=[nn_neptune_callback])\n",
        "    joblib.dump(\n",
        "        study, f'hpo/params/SupAEnn_hpo_{str(datetime.datetime.now().date())}.pkl')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}