{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import datetime\n",
        "import gc\n",
        "import copy\n",
        "import datatable as dt\n",
        "import joblib\n",
        "import lightgbm as lgb\n",
        "import neptune\n",
        "import neptunecontrib.monitoring.optuna as opt_utils\n",
        "import numpy as np\n",
        "import optuna\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from data_loading import utils\n",
        "from data_loading import purged_group_time_series as pgs\n",
        "\n",
        "\n",
        "def optimize(trial: optuna.trial.Trial, data_dict: dict):\n",
        "    p = {'learning_rate':    trial.suggest_uniform('learning_rate', 1e-4, 1e-1),\n",
        "         'max_depth':        trial.suggest_int('max_depth', 5, 30),\n",
        "         'max_leaves':       trial.suggest_int('max_leaves', 5, 50),\n",
        "         'subsample':        trial.suggest_uniform('subsample', 0.3, 1.0),\n",
        "         'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.3, 1.0),\n",
        "         'min_child_weight': trial.suggest_int('min_child_weight', 5, 100),\n",
        "         'lambda':           trial.suggest_uniform('lambda', 0.05, 0.2),\n",
        "         'alpha':            trial.suggest_uniform('alpha', 0.05, 0.2),\n",
        "         'objective':        'reg:squarederror',\n",
        "         'booster':          'gbtree',\n",
        "         'tree_method':      'hist',\n",
        "         'verbosity':        1,\n",
        "         'n_jobs':           4,\n",
        "         'eval_metric':      'rmse'}\n",
        "    print('Choosing parameters:', p)\n",
        "    scores = []\n",
        "    sizes = []\n",
        "    # gts = GroupTimeSeriesSplit()']\n",
        "\n",
        "    gts = pgs.PurgedGroupTimeSeriesSplit(n_splits=5, group_gap=10)\n",
        "    for i, (tr_idx, val_idx) in enumerate(gts.split(data_dict['data'], groups=data_dict['era'])):\n",
        "        x_tr, x_val = data_dict['data'][tr_idx], data_dict['data'][val_idx]\n",
        "        y_tr, y_val = data_dict['target'][tr_idx], data_dict['target'][val_idx]\n",
        "        d_tr = xgb.DMatrix(x_tr, label=y_tr)\n",
        "        d_val = xgb.DMatrix(x_val, label=y_val)\n",
        "        clf = xgb.train(p, d_tr, 500, [\n",
        "            (d_val, 'eval')], early_stopping_rounds=50, verbose_eval=True)\n",
        "        val_pred = clf.predict(d_val)\n",
        "        score = mean_squared_error(y_val, val_pred)\n",
        "        scores.append(score)\n",
        "        sizes.append(len(tr_idx) + len(val_idx))\n",
        "        del clf, val_pred, d_tr, d_val, x_tr, x_val, y_tr, y_val, score\n",
        "        rubbish = gc.collect()\n",
        "    print(scores)\n",
        "    avg_score = utils.weighted_mean(scores, sizes)\n",
        "    print('Avg Score:', avg_score)\n",
        "    return avg_score\n",
        "\n",
        "\n",
        "def loptimize(trial, data_dict: dict):\n",
        "    p = {'learning_rate':    trial.suggest_uniform('learning_rate', 1e-4, 1e-1),\n",
        "         'max_leaves':       trial.suggest_int('max_leaves', 5, 100),\n",
        "         'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.3, 0.99),\n",
        "         'bagging_freq':     trial.suggest_int('bagging_freq', 1, 10),\n",
        "         'feature_fraction': trial.suggest_uniform('feature_fraction', 0.3, 0.99),\n",
        "         'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 50, 1000),\n",
        "         'lambda_l1':        trial.suggest_uniform('lambda_l1', 0.005, 0.05),\n",
        "         'lambda_l2':        trial.suggest_uniform('lambda_l2', 0.005, 0.05),\n",
        "         'boosting':         trial.suggest_categorical('boosting', ['gbdt', 'goss', 'rf']),\n",
        "         'objective':        'regression',\n",
        "         'verbose':          1,\n",
        "         'n_jobs':           4,\n",
        "         'metric':           'mse'}\n",
        "    if p['boosting'] == 'goss':\n",
        "        p['bagging_freq'] = 0\n",
        "        p['bagging_fraction'] = 1.0\n",
        "    scores = []\n",
        "    sizes = []\n",
        "    # gts = GroupTimeSeriesSplit()\n",
        "    gts = pgs.PurgedGroupTimeSeriesSplit(n_splits=5, group_gap=10)\n",
        "    for i, (tr_idx, val_idx) in enumerate(gts.split(data_dict['data'], groups=data_dict['era'])):\n",
        "        sizes.append(len(tr_idx) + len(val_idx))\n",
        "        x_tr, x_val = data_dict['data'][tr_idx], data_dict['data'][val_idx]\n",
        "        y_tr, y_val = data_dict['target'][tr_idx], data_dict['target'][val_idx]\n",
        "        train = lgb.Dataset(x_tr, label=y_tr)\n",
        "        val = lgb.Dataset(x_val, label=y_val)\n",
        "        clf = lgb.train(p, train, 500, valid_sets=[\n",
        "            val], early_stopping_rounds=50, verbose_eval=True)\n",
        "        preds = clf.predict(x_val)\n",
        "        score = mean_squared_error(y_val, preds)\n",
        "        scores.append(score)\n",
        "        del clf, preds, train, val, x_tr, x_val, y_tr, y_val, score\n",
        "        rubbish = gc.collect()\n",
        "    print(scores)\n",
        "    avg_score = utils.weighted_mean(scores, sizes)\n",
        "    print('Avg Score:', avg_score)\n",
        "    return avg_score\n",
        "\n",
        "\n",
        "def main():\n",
        "    api_token = 'eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzMGQ1MWZiNy1iYjNlLTQ3NDctOTE4OS1lNzhlNmVlYmUwMzYifQ=='\n",
        "    neptune.init(api_token=api_token,\n",
        "                 project_qualified_name='kramerji/Numerai')\n",
        "    data = utils.load_data('data/', mode='train')\n",
        "    data, target, features, era = utils.preprocess_data(data, nn=True)\n",
        "    data_dict = {'data':     data, 'target': target,\n",
        "                 'features': features, 'era': era}\n",
        "    print('creating XGBoost Trials')\n",
        "    xgb_exp = neptune.create_experiment('XGBoost_HPO')\n",
        "    xgb_neptune_callback = opt_utils.NeptuneCallback(experiment=xgb_exp)\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    study.optimize(lambda trial: optimize(trial, data_dict),\n",
        "                   n_trials=100, callbacks=[xgb_neptune_callback])\n",
        "    joblib.dump(\n",
        "        study, f'hpo/params/xgb_hpo_{str(datetime.datetime.now().date())}.pkl')\n",
        "    print('Creating LightGBM Trials')\n",
        "    lgb_exp = neptune.create_experiment('LGBM_HPO')\n",
        "    lgbm_neptune_callback = opt_utils.NeptuneCallback(experiment=lgb_exp)\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    study.optimize(lambda trial: loptimize(trial, data_dict),\n",
        "                   n_trials=100, callbacks=[lgbm_neptune_callback])\n",
        "    joblib.dump(\n",
        "        study, f'hpo/params/lgb_hpo_{str(datetime.datetime.now().date())}.pkl')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}