{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from logging import root\n",
        "from models.SupervisedAutoEncoder import create_hidden_rep\n",
        "from operator import mod\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from typing import List, Tuple\n",
        "import dotenv\n",
        "import datatable as dt\n",
        "from dotenv.main import load_dotenv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
        "from torch.utils.data import Dataset, Subset, BatchSampler, SequentialSampler, DataLoader\n",
        "\n",
        "\n",
        "# from lightning_nn import Classifier\n",
        "\n",
        "\n",
        "class FinData(Dataset):\n",
        "    def __init__(self, data, target, era, hidden=None, mode='train', transform=None, cache_dir=None):\n",
        "        self.data = data\n",
        "        self.target = target\n",
        "        self.mode = mode\n",
        "        self.transform = transform\n",
        "        self.cache_dir = cache_dir\n",
        "        self.era = era\n",
        "        self.hidden = hidden\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index.to_list()\n",
        "        if self.transform:\n",
        "            return self.transform(self.data.iloc[index].values)\n",
        "        else:\n",
        "            if type(index) is list:\n",
        "                sample = {\n",
        "                    'target': torch.Tensor(self.target[index].values),\n",
        "                    'data':   torch.LongTensor(self.data[index]),\n",
        "                    'era':    torch.Tensor(self.era[index].values),\n",
        "                }\n",
        "            else:\n",
        "                sample = {\n",
        "                    'target': torch.Tensor([self.target[index]]),\n",
        "                    'data':   torch.LongTensor([self.data[index]]),\n",
        "                    'era':    torch.Tensor([self.era[index]]),\n",
        "                }\n",
        "            if self.hidden is not None:\n",
        "                sample['hidden'] = torch.Tensor(self.hidden['hidden'][index])\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def get_data_path(root_dir):\n",
        "    dotenv_path = 'num_config.env'\n",
        "    load_dotenv(dotenv_path=dotenv_path)\n",
        "    curr_round = os.getenv('LATEST_ROUND')\n",
        "    data_path = root_dir + '/numerai_dataset_' + str(curr_round)\n",
        "    return data_path\n",
        "\n",
        "\n",
        "def load_data(root_dir, mode, overide=None):\n",
        "    data_path = get_data_path(root_dir=root_dir)\n",
        "    if overide:\n",
        "        data = dt.fread(overide).to_pandas()\n",
        "    elif mode == 'train':\n",
        "        data = dt.fread(data_path + '/numerai_training_data.csv').to_pandas()\n",
        "    elif mode == 'test':\n",
        "        data = dt.fread(data_path + '/numerai_tournament_data.csv').to_pandas()\n",
        "    return data\n",
        "\n",
        "\n",
        "def preprocess_data(data: pd.DataFrame, scale: bool = False, nn: bool = False, test=False, ordinal=False):\n",
        "    \"\"\"\n",
        "    Preprocess the data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data\n",
        "        Pandas DataFrame\n",
        "    scale\n",
        "        scale data with unit std and 0 mean\n",
        "    nn\n",
        "        return data as np.array\n",
        "    missing\n",
        "        options to replace missing data with - mean, median, 0\n",
        "    action\n",
        "        options to create action value  - weight = (weight * resp) > 0\n",
        "                                        - combined = (resp_cols) > 0\n",
        "                                        - multi = each resp cols >0\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    \"\"\"\n",
        "    features = [col for col in data.columns if 'feature' in col]\n",
        "    era = data['era']\n",
        "    era = era.transform(lambda x: re.sub('[a-z]', '', x))\n",
        "    if not test:\n",
        "        era = era.astype('int')\n",
        "    target = data['target']\n",
        "    data = data[features]\n",
        "    if scale:\n",
        "        scaler = StandardScaler()\n",
        "        data = scaler.fit_transform(data)\n",
        "    if ordinal:\n",
        "        oe = OrdinalEncoder()\n",
        "        data = oe.fit_transform(data)\n",
        "        # data = data.values\n",
        "    if nn:\n",
        "        data = data.values\n",
        "    return data, target, features, era\n",
        "\n",
        "\n",
        "def calc_data_mean(array, cache_dir=None, fold=None, train=True, mode='mean'):\n",
        "    if train:\n",
        "        if mode == 'mean':\n",
        "            f_mean = np.nanmean(array, axis=0)\n",
        "            if cache_dir and fold:\n",
        "                np.save(f'{cache_dir}/f_{fold}_mean.npy', f_mean)\n",
        "            elif cache_dir:\n",
        "                np.save(f'{cache_dir}/f_mean.npy', f_mean)\n",
        "            array = np.nan_to_num(array) + np.isnan(array) * f_mean\n",
        "        if mode == 'median':\n",
        "            f_med = np.nanmedian(array, axis=0)\n",
        "            if cache_dir and fold:\n",
        "                np.save(f'{cache_dir}/f_{fold}_median.npy', f_med)\n",
        "            elif cache_dir:\n",
        "                np.save(f'{cache_dir}/f_median.npy', f_med)\n",
        "            array = np.nan_to_num(array) + np.isnan(array) * f_med\n",
        "        if mode == 'zero':\n",
        "            array = np.nan_to_num(array) + np.isnan(array) * 0\n",
        "    if not train:\n",
        "        if mode == 'mean':\n",
        "            f_mean = np.load(f'{cache_dir}/f_mean.npy')\n",
        "            array = np.nan_to_num(array) + np.isnan(array) * f_mean\n",
        "        if mode == 'median':\n",
        "            f_med = np.load(f'{cache_dir}/f_med.npy')\n",
        "            array = np.nan_to_num(array) + np.isnan(array) * f_med\n",
        "        if mode == 'zero':\n",
        "            array = np.nan_to_num(array) + np.isnan(array) * 0\n",
        "    return array\n",
        "\n",
        "\n",
        "def weighted_mean(scores, sizes):\n",
        "    largest = np.max(sizes)\n",
        "    weights = [size / largest for size in sizes]\n",
        "    return np.average(scores, weights=weights)\n",
        "\n",
        "\n",
        "def create_dataloaders(dataset: Dataset, indexes: dict, batch_size):\n",
        "    train_idx = indexes.get('train', None)\n",
        "    val_idx = indexes.get('val', None)\n",
        "    test_idx = indexes.get('test', None)\n",
        "    dataloaders = {}\n",
        "    if train_idx:\n",
        "        train_set = Subset(dataset, train_idx)\n",
        "        train_sampler = BatchSampler(\n",
        "            train_set.indices, batch_size=batch_size, drop_last=False)\n",
        "        dataloaders['train'] = DataLoader(\n",
        "            dataset, sampler=train_sampler, num_workers=2, pin_memory=False, shuffle=False)\n",
        "    if val_idx:\n",
        "        val_set = Subset(dataset, val_idx)\n",
        "        val_sampler = BatchSampler(\n",
        "            val_set.indices, batch_size=batch_size, drop_last=False)\n",
        "        dataloaders['val'] = DataLoader(\n",
        "            dataset, sampler=val_sampler, num_workers=2, pin_memory=False, shuffle=False)\n",
        "    if test_idx:\n",
        "        test_set = Subset(dataset, test_idx)\n",
        "        test_sampler = BatchSampler(\n",
        "            test_set.indices, batch_size=batch_size, drop_last=False)\n",
        "        dataloaders['test'] = DataLoader(\n",
        "            dataset, sampler=test_sampler, num_workers=2, pin_memory=False, shuffle=False)\n",
        "    return dataloaders\n",
        "\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "def load_model(path, p, pl_lightning, model):\n",
        "    Classifier = model\n",
        "    if os.path.isdir(path):\n",
        "        models = []\n",
        "        for file in os.listdir(path):\n",
        "            if pl_lightning:\n",
        "                model = Classifier.load_from_checkpoint(\n",
        "                    checkpoint_path=path + '/' + file, params=p)\n",
        "            else:\n",
        "                model = Classifier(params=p)\n",
        "                model.load_state_dict(torch.load(path + '/' + file))\n",
        "            models.append(model)\n",
        "        return models\n",
        "    elif os.path.isfile(path):\n",
        "        if pl_lightning:\n",
        "            return Classifier.load_from_checkpoint(checkpoint_path=path, params=p)\n",
        "        else:\n",
        "            model = Classifier(params=p)\n",
        "            model.load_state_dict(torch.load(path))\n",
        "            return model\n",
        "\n",
        "\n",
        "def init_weights(m, func):\n",
        "    if type(m) == nn.Linear:\n",
        "        nn.init.xavier_normal_(m.weight, nn.init.calculate_gain(func))\n",
        "\n",
        "\n",
        "def create_predictions(root_dir: str = './data', models: dict = {}, hidden=True, ae=True):\n",
        "    test_files_path, test_files_exist = check_test_files(root_dir)\n",
        "    if test_files_exist:\n",
        "        test_files = os.listdir(test_files_path)\n",
        "    for file in test_files:\n",
        "        df = load_data(root_dir='./data', mode='test',\n",
        "                       overide=f'{test_files_path}/{file}')\n",
        "        df['target'] = 0\n",
        "        data, target, features, era = preprocess_data(data=df, ordinal=True)\n",
        "        t_idx = np.arange(start=0, stop=len(era), step=1).tolist()\n",
        "        data_dict = data_dict = {'data': data, 'target': target,\n",
        "                                 'features': features, 'era': era}\n",
        "        if models.get('ae', None):\n",
        "            p_ae = models['ae'][1]\n",
        "            p_ae['input_size'] = len(features)\n",
        "            p_ae['output_size'] = 1\n",
        "            model = models['ae'][0]\n",
        "            model.eval()\n",
        "        if not ae:\n",
        "            hidden_pred = create_hidden_rep(\n",
        "                model=model, data_dict=data_dict)\n",
        "            data_dict['hidden_true'] = True\n",
        "            df['prediction_ae'] = hidden_pred['preds']\n",
        "        if models.get('ResNet', None):\n",
        "            p_res = models['ResNet'][1]\n",
        "            p_res['input_size'] = len(features)\n",
        "            p_res['output_size'] = 1\n",
        "            p_res['hidden_len'] = data_dict['hidden'].shape[-1]\n",
        "            dataset = FinData(\n",
        "                data=data_dict['data'], target=data_dict['target'], era=data_dict['era'], hidden=data_dict.get('hidden', None))\n",
        "            dataloaders = create_dataloaders(\n",
        "                dataset, indexes={'train': t_idx}, batch_size=p_res['batch_size'])\n",
        "            model = models['ResNet'][0]\n",
        "            model.eval()\n",
        "            predictions = []\n",
        "            for batch in dataloaders['train']:\n",
        "                pred = model(batch['data'].view(\n",
        "                    batch['data'].shape[1], -1), hidden=batch['hidden'].view(batch['hidden'].shape[1], -1))\n",
        "                predictions.append(pred.cpu().detach().numpy().tolist())\n",
        "            predictions = np.array([predictions[i][j] for i in range(\n",
        "                len(predictions)) for j in range(len(predictions[i]))])\n",
        "            df['prediction_resnet'] = predictions\n",
        "        if models.get('xgboost', None):\n",
        "            model_xgboost = models['xgboost'][0]\n",
        "            p_xgboost = models['xgboost'][1]\n",
        "            x_val = data_dict['data']\n",
        "            df['prediction_xgb'] = model_xgboost.predict(x_val)\n",
        "        if models.get('lgb', None):\n",
        "            model_lgb = models['lgb'][0]\n",
        "            p_lgb = models['lgb'][1]\n",
        "            x_val = data_dict['data']\n",
        "            df['prediction_lgb'] = model_lgb.predict(x_val)\n",
        "            df = df[['id', 'prediction_lgb']]\n",
        "        pred_path = f'{get_data_path(root_dir)}/predictions/{era[0]}'\n",
        "        df.to_csv(f'{pred_path}.csv')\n",
        "\n",
        "\n",
        "def check_test_files(root_dir='./data'):\n",
        "    data_path = get_data_path(root_dir)\n",
        "    test_files_path = f'{data_path}/test_files'\n",
        "    # TODO Check to make sure all era's present\n",
        "    if os.path.isdir(test_files_path):\n",
        "        return test_files_path, True\n",
        "    else:\n",
        "        os.makedirs(test_files_path)\n",
        "        df = load_data(root_dir=root_dir, mode='test')\n",
        "        df['era'][df['era'] == 'eraX'] = 'era999'\n",
        "        for era in df['era'].unique():\n",
        "            path = f'{test_files_path}/{era}'\n",
        "            df[df['era'] == era].to_csv(f'{path}.csv')\n",
        "        return test_files_path, True\n",
        "\n",
        "\n",
        "def create_prediction_file(root_dir='./data', eras=None):\n",
        "    pred_path = f'{get_data_path(root_dir)}/predictions/'\n",
        "    files = os.listdir(pred_path)\n",
        "    files.sort()\n",
        "    if eras:\n",
        "        dfs = [pd.read_csv(f'{pred_path}{file}')\n",
        "               for file in files if file != 'predictions.csv' and file in eras]\n",
        "    else:\n",
        "        dfs = [pd.read_csv(f'{pred_path}{file}')\n",
        "               for file in files if file != 'predictions.csv']\n",
        "    df = pd.concat(dfs)\n",
        "    df = df[['id', 'prediction_lgb']]\n",
        "    df.columns = ['id', 'prediction']\n",
        "    df.to_csv(f'{pred_path}predictions.csv')\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import joblib\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from data_loading import utils\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "\n",
        "\n",
        "class SupAE(pl.LightningModule):\n",
        "    def __init__(self, params):\n",
        "        super(SupAE, self).__init__()\n",
        "        self.lr = params['lr']\n",
        "        self.loss_recon = params['loss_recon']()\n",
        "        self.recon_loss_factor = params['recon_loss_factor']\n",
        "        self.loss_sup_ae = params['loss_sup_ae']()\n",
        "        self.activation = params['activation']\n",
        "        self.drop = params['dropout']\n",
        "        cat_dims = [5 for i in range(params['input_size'])]\n",
        "        emb_dims = [(x, min(50, (x + 1) // 2)) for x in cat_dims]\n",
        "        self.embedding_layers = nn.ModuleList(\n",
        "            [nn.Embedding(x, y) for x, y in emb_dims]).to(self.device)\n",
        "        self.num_embeddings = sum([y for x, y in emb_dims])\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(self.num_embeddings, params['dim_1']),\n",
        "            nn.BatchNorm1d(params['dim_1']),\n",
        "            self.activation(),\n",
        "            nn.Dropout(self.drop),\n",
        "            nn.Linear(params['dim_1'], params['dim_2']),\n",
        "            nn.BatchNorm1d(params['dim_2']),\n",
        "            self.activation(),\n",
        "            nn.Dropout(self.drop),\n",
        "            nn.Linear(params['dim_2'], params['dim_3']),\n",
        "            nn.BatchNorm1d(params['dim_3']),\n",
        "            self.activation(),\n",
        "            nn.Dropout(self.drop),\n",
        "            nn.Linear(params['dim_3'], params['hidden'])\n",
        "        )\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.BatchNorm1d(params['hidden']),\n",
        "            self.activation(),\n",
        "            nn.Dropout(self.drop),\n",
        "            nn.Linear(params['hidden'], params['output_size'])\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(params['hidden'], params['dim_3']),\n",
        "            nn.BatchNorm1d(params['dim_3']),\n",
        "            self.activation(),\n",
        "            nn.Dropout(self.drop),\n",
        "            nn.Linear(params['dim_3'], params['dim_2']),\n",
        "            nn.BatchNorm1d(params['dim_2']),\n",
        "            self.activation(),\n",
        "            nn.Dropout(self.drop),\n",
        "            nn.Linear(params['dim_2'], params['dim_1']),\n",
        "            nn.BatchNorm1d(params['dim_1']),\n",
        "            self.activation(),\n",
        "            nn.Dropout(self.drop),\n",
        "            nn.Linear(params['dim_1'], self.num_embeddings)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = [emb_lay(x[:, i])\n",
        "             for i, emb_lay in enumerate(self.embedding_layers)]\n",
        "        emb = torch.cat(x, 1)\n",
        "        hidden = self.encoder(emb)\n",
        "        reg_out = self.regressor(hidden)\n",
        "        decoder_out = self.decoder(hidden)\n",
        "        return emb, hidden, reg_out, decoder_out\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch['data'], batch['target']\n",
        "        x = x.view(x.size(1), -1)\n",
        "        y = y.T\n",
        "        emb, _, reg_out, decoder_out = self(x)\n",
        "        sup_loss = self.loss_sup_ae(reg_out, y)\n",
        "        recon_loss = torch.mean(torch.tensor(\n",
        "            [self.loss_recon(decoder_out[i], emb[i]) for i in range(x.shape[0])]))\n",
        "        loss = sup_loss + self.recon_loss_factor*recon_loss\n",
        "        self.log('sup_loss', sup_loss, on_step=False,\n",
        "                 on_epoch=True, prog_bar=True)\n",
        "        self.log('recon_loss', recon_loss, on_step=False,\n",
        "                 on_epoch=True, prog_bar=True)\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        return {'loss': loss}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch['data'], batch['target']\n",
        "        x = x.view(x.size(1), -1)\n",
        "        y = y.T\n",
        "        emb, _, reg_out, decoder_out = self(x)\n",
        "        sup_loss = self.loss_sup_ae(reg_out, y)\n",
        "        recon_loss = torch.mean(torch.tensor(\n",
        "            [self.loss_recon(decoder_out[i], emb[i]) for i in range(x.shape[0])]))\n",
        "        loss = sup_loss + self.recon_loss_factor*recon_loss\n",
        "        \"\"\"\n",
        "        self.log('sup_loss', sup_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
        "        self.log('recon_loss', recon_loss, on_step=False, on_epoch=True, prog_bar=True)\n",
        "        self.log('loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
        "        \"\"\"\n",
        "        return {'val_loss': loss, 'val_sup_loss': sup_loss}\n",
        "\n",
        "    def validation_epoch_end(self, outputs) -> None:\n",
        "        epoch_loss = torch.tensor([x['val_loss'] for x in outputs]).mean()\n",
        "        epoch_sup_loss = torch.tensor(\n",
        "            [x['val_sup_loss'] for x in outputs]).mean()\n",
        "        self.log('val_loss', epoch_loss, prog_bar=True)\n",
        "        self.log('val_sup_loss', epoch_sup_loss, prog_bar=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(\n",
        "            self.parameters(), lr=self.lr)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, patience=5, factor=0.1, min_lr=1e-7, eps=1e-08\n",
        "        )\n",
        "        return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'val_loss'}\n",
        "\n",
        "\n",
        "def train_ae_model(data_dict):\n",
        "    # TODO Dynamic\n",
        "    p = {'dim_1': 675, 'dim_2': 400, 'dim_3': 224, 'hidden': 162,\n",
        "         'activation': nn.ReLU, 'dropout': 0.2916447561918717, 'lr': 0.030272591341587315,\n",
        "         'recon_loss_factor': 0.4447516076774931, 'batch_size': 1252, 'loss_sup_ae': nn.MSELoss,\n",
        "         'loss_recon': nn.MSELoss,\n",
        "         'embedding': True}\n",
        "    # TODO Fix this\n",
        "    joblib.dump(p,'./saved_models/parameters/ae_params.pkl')\n",
        "    train_idx = np.arange(start=0, stop=452205, step=1, dtype=np.int).tolist()\n",
        "    val_idx = np.arange(start=452206, stop=len(\n",
        "        data_dict['data']), step=1, dtype=np.int).tolist()\n",
        "    p['input_size'] = len(data_dict['features'])\n",
        "    p['output_size'] = 1\n",
        "    dataset = utils.FinData(\n",
        "        data=data_dict['data'], target=data_dict['target'], era=data_dict['era'])\n",
        "    dataloaders = utils.create_dataloaders(dataset=dataset, indexes={\n",
        "                                           'train': train_idx, 'val': val_idx}, batch_size=p['batch_size'])\n",
        "    model = SupAE(p)\n",
        "    es = EarlyStopping(monitor='val_loss', patience=10,\n",
        "                       min_delta=0.005, mode='min')\n",
        "    trainer = pl.Trainer(max_epochs=100,\n",
        "                         gpus=0,\n",
        "                         callbacks=[es])\n",
        "    trainer.fit(\n",
        "        model, train_dataloader=dataloaders['train'], val_dataloaders=dataloaders['val'])\n",
        "    torch.save(model.state_dict(), f'./saved_models/trained/trained_ae.pth')\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_hidden_rep(model, data_dict):\n",
        "    model.eval()\n",
        "    index = np.linspace(\n",
        "        0, data_dict['data'].shape[0]-1, data_dict['data'].shape[0], dtype='int').tolist()\n",
        "    dataset = utils.FinData(\n",
        "        data_dict['data'], target=data_dict['target'], era=data_dict['era'])\n",
        "    batch_size = 5000\n",
        "    dataloaders = utils.create_dataloaders(\n",
        "        dataset, {'train': index}, batch_size=batch_size)\n",
        "    hiddens = []\n",
        "    predictions = []\n",
        "    for i, batch in enumerate(dataloaders['train']):\n",
        "        _, hidden, pred, _ = model(batch['data'].view(\n",
        "            batch['data'].size(1), -1))\n",
        "        hiddens.append(hidden.cpu().detach().numpy().tolist())\n",
        "        predictions.append(pred.cpu().detach().numpy().tolist())\n",
        "    hiddens = np.array([hiddens[i][j] for i in range(\n",
        "        len(hiddens)) for j in range(len(hiddens[i]))])\n",
        "    preds = np.array([predictions[i][j] for i in range(\n",
        "        len(predictions)) for j in range(len(predictions[i]))])\n",
        "    return {'hidden': hiddens, 'preds': preds}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from logging import root\n",
        "from models.SupervisedAutoEncoder import create_hidden_rep\n",
        "from operator import mod\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from typing import List, Tuple\n",
        "import dotenv\n",
        "import datatable as dt\n",
        "from dotenv.main import load_dotenv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
        "from torch.utils.data import Dataset, Subset, BatchSampler, SequentialSampler, DataLoader\n",
        "\n",
        "\n",
        "# from lightning_nn import Classifier\n",
        "\n",
        "\n",
        "class FinData(Dataset):\n",
        "    def __init__(self, data, target, era, hidden=None, mode='train', transform=None, cache_dir=None):\n",
        "        self.data = data\n",
        "        self.target = target\n",
        "        self.mode = mode\n",
        "        self.transform = transform\n",
        "        self.cache_dir = cache_dir\n",
        "        self.era = era\n",
        "        self.hidden = hidden\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index.to_list()\n",
        "        if self.transform:\n",
        "            return self.transform(self.data.iloc[index].values)\n",
        "        else:\n",
        "            if type(index) is list:\n",
        "                sample = {\n",
        "                    'target': torch.Tensor(self.target[index].values),\n",
        "                    'data':   torch.LongTensor(self.data[index]),\n",
        "                    'era':    torch.Tensor(self.era[index].values),\n",
        "                }\n",
        "            else:\n",
        "                sample = {\n",
        "                    'target': torch.Tensor([self.target[index]]),\n",
        "                    'data':   torch.LongTensor([self.data[index]]),\n",
        "                    'era':    torch.Tensor([self.era[index]]),\n",
        "                }\n",
        "            if self.hidden is not None:\n",
        "                sample['hidden'] = torch.Tensor(self.hidden['hidden'][index])\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def get_data_path(root_dir):\n",
        "    dotenv_path = 'num_config.env'\n",
        "    load_dotenv(dotenv_path=dotenv_path)\n",
        "    curr_round = os.getenv('LATEST_ROUND')\n",
        "    data_path = root_dir + '/numerai_dataset_' + str(curr_round)\n",
        "    return data_path\n",
        "\n",
        "\n",
        "def load_data(root_dir, mode, overide=None):\n",
        "    data_path = get_data_path(root_dir=root_dir)\n",
        "    if overide:\n",
        "        data = dt.fread(overide).to_pandas()\n",
        "    elif mode == 'train':\n",
        "        data = dt.fread(data_path + '/numerai_training_data.csv').to_pandas()\n",
        "    elif mode == 'test':\n",
        "        data = dt.fread(data_path + '/numerai_tournament_data.csv').to_pandas()\n",
        "    return data\n",
        "\n",
        "\n",
        "def preprocess_data(data: pd.DataFrame, scale: bool = False, nn: bool = False, test=False, ordinal=False):\n",
        "    \"\"\"\n",
        "    Preprocess the data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data\n",
        "        Pandas DataFrame\n",
        "    scale\n",
        "        scale data with unit std and 0 mean\n",
        "    nn\n",
        "        return data as np.array\n",
        "    missing\n",
        "        options to replace missing data with - mean, median, 0\n",
        "    action\n",
        "        options to create action value  - weight = (weight * resp) > 0\n",
        "                                        - combined = (resp_cols) > 0\n",
        "                                        - multi = each resp cols >0\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    \"\"\"\n",
        "    features = [col for col in data.columns if 'feature' in col]\n",
        "    era = data['era']\n",
        "    era = era.transform(lambda x: re.sub('[a-z]', '', x))\n",
        "    if not test:\n",
        "        era = era.astype('int')\n",
        "    target = data['target']\n",
        "    data = data[features]\n",
        "    if scale:\n",
        "        scaler = StandardScaler()\n",
        "        data = scaler.fit_transform(data)\n",
        "    if ordinal:\n",
        "        oe = OrdinalEncoder()\n",
        "        data = oe.fit_transform(data)\n",
        "        # data = data.values\n",
        "    if nn:\n",
        "        data = data.values\n",
        "    return data, target, features, era\n",
        "\n",
        "\n",
        "def calc_data_mean(array, cache_dir=None, fold=None, train=True, mode='mean'):\n",
        "    if train:\n",
        "        if mode == 'mean':\n",
        "            f_mean = np.nanmean(array, axis=0)\n",
        "            if cache_dir and fold:\n",
        "                np.save(f'{cache_dir}/f_{fold}_mean.npy', f_mean)\n",
        "            elif cache_dir:\n",
        "                np.save(f'{cache_dir}/f_mean.npy', f_mean)\n",
        "            array = np.nan_to_num(array) + np.isnan(array) * f_mean\n",
        "        if mode == 'median':\n",
        "            f_med = np.nanmedian(array, axis=0)\n",
        "            if cache_dir and fold:\n",
        "                np.save(f'{cache_dir}/f_{fold}_median.npy', f_med)\n",
        "            elif cache_dir:\n",
        "                np.save(f'{cache_dir}/f_median.npy', f_med)\n",
        "            array = np.nan_to_num(array) + np.isnan(array) * f_med\n",
        "        if mode == 'zero':\n",
        "            array = np.nan_to_num(array) + np.isnan(array) * 0\n",
        "    if not train:\n",
        "        if mode == 'mean':\n",
        "            f_mean = np.load(f'{cache_dir}/f_mean.npy')\n",
        "            array = np.nan_to_num(array) + np.isnan(array) * f_mean\n",
        "        if mode == 'median':\n",
        "            f_med = np.load(f'{cache_dir}/f_med.npy')\n",
        "            array = np.nan_to_num(array) + np.isnan(array) * f_med\n",
        "        if mode == 'zero':\n",
        "            array = np.nan_to_num(array) + np.isnan(array) * 0\n",
        "    return array\n",
        "\n",
        "\n",
        "def weighted_mean(scores, sizes):\n",
        "    largest = np.max(sizes)\n",
        "    weights = [size / largest for size in sizes]\n",
        "    return np.average(scores, weights=weights)\n",
        "\n",
        "\n",
        "def create_dataloaders(dataset: Dataset, indexes: dict, batch_size):\n",
        "    train_idx = indexes.get('train', None)\n",
        "    val_idx = indexes.get('val', None)\n",
        "    test_idx = indexes.get('test', None)\n",
        "    dataloaders = {}\n",
        "    if train_idx:\n",
        "        train_set = Subset(dataset, train_idx)\n",
        "        train_sampler = BatchSampler(\n",
        "            train_set.indices, batch_size=batch_size, drop_last=False)\n",
        "        dataloaders['train'] = DataLoader(\n",
        "            dataset, sampler=train_sampler, num_workers=2, pin_memory=False, shuffle=False)\n",
        "    if val_idx:\n",
        "        val_set = Subset(dataset, val_idx)\n",
        "        val_sampler = BatchSampler(\n",
        "            val_set.indices, batch_size=batch_size, drop_last=False)\n",
        "        dataloaders['val'] = DataLoader(\n",
        "            dataset, sampler=val_sampler, num_workers=2, pin_memory=False, shuffle=False)\n",
        "    if test_idx:\n",
        "        test_set = Subset(dataset, test_idx)\n",
        "        test_sampler = BatchSampler(\n",
        "            test_set.indices, batch_size=batch_size, drop_last=False)\n",
        "        dataloaders['test'] = DataLoader(\n",
        "            dataset, sampler=test_sampler, num_workers=2, pin_memory=False, shuffle=False)\n",
        "    return dataloaders\n",
        "\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "def load_model(path, p, pl_lightning, model):\n",
        "    Classifier = model\n",
        "    if os.path.isdir(path):\n",
        "        models = []\n",
        "        for file in os.listdir(path):\n",
        "            if pl_lightning:\n",
        "                model = Classifier.load_from_checkpoint(\n",
        "                    checkpoint_path=path + '/' + file, params=p)\n",
        "            else:\n",
        "                model = Classifier(params=p)\n",
        "                model.load_state_dict(torch.load(path + '/' + file))\n",
        "            models.append(model)\n",
        "        return models\n",
        "    elif os.path.isfile(path):\n",
        "        if pl_lightning:\n",
        "            return Classifier.load_from_checkpoint(checkpoint_path=path, params=p)\n",
        "        else:\n",
        "            model = Classifier(params=p)\n",
        "            model.load_state_dict(torch.load(path))\n",
        "            return model\n",
        "\n",
        "\n",
        "def init_weights(m, func):\n",
        "    if type(m) == nn.Linear:\n",
        "        nn.init.xavier_normal_(m.weight, nn.init.calculate_gain(func))\n",
        "\n",
        "\n",
        "def create_predictions(root_dir: str = './data', models: dict = {}, hidden=True, ae=True):\n",
        "    test_files_path, test_files_exist = check_test_files(root_dir)\n",
        "    if test_files_exist:\n",
        "        test_files = os.listdir(test_files_path)\n",
        "    for file in test_files:\n",
        "        df = load_data(root_dir='./data', mode='test',\n",
        "                       overide=f'{test_files_path}/{file}')\n",
        "        df['target'] = 0\n",
        "        data, target, features, era = preprocess_data(data=df, ordinal=True)\n",
        "        t_idx = np.arange(start=0, stop=len(era), step=1).tolist()\n",
        "        data_dict = data_dict = {'data': data, 'target': target,\n",
        "                                 'features': features, 'era': era}\n",
        "        if models.get('ae', None):\n",
        "            p_ae = models['ae'][1]\n",
        "            p_ae['input_size'] = len(features)\n",
        "            p_ae['output_size'] = 1\n",
        "            model = models['ae'][0]\n",
        "            model.eval()\n",
        "        if not ae:\n",
        "            hidden_pred = create_hidden_rep(\n",
        "                model=model, data_dict=data_dict)\n",
        "            data_dict['hidden_true'] = True\n",
        "            df['prediction_ae'] = hidden_pred['preds']\n",
        "        if models.get('ResNet', None):\n",
        "            p_res = models['ResNet'][1]\n",
        "            p_res['input_size'] = len(features)\n",
        "            p_res['output_size'] = 1\n",
        "            p_res['hidden_len'] = data_dict['hidden'].shape[-1]\n",
        "            dataset = FinData(\n",
        "                data=data_dict['data'], target=data_dict['target'], era=data_dict['era'], hidden=data_dict.get('hidden', None))\n",
        "            dataloaders = create_dataloaders(\n",
        "                dataset, indexes={'train': t_idx}, batch_size=p_res['batch_size'])\n",
        "            model = models['ResNet'][0]\n",
        "            model.eval()\n",
        "            predictions = []\n",
        "            for batch in dataloaders['train']:\n",
        "                pred = model(batch['data'].view(\n",
        "                    batch['data'].shape[1], -1), hidden=batch['hidden'].view(batch['hidden'].shape[1], -1))\n",
        "                predictions.append(pred.cpu().detach().numpy().tolist())\n",
        "            predictions = np.array([predictions[i][j] for i in range(\n",
        "                len(predictions)) for j in range(len(predictions[i]))])\n",
        "            df['prediction_resnet'] = predictions\n",
        "        if models.get('xgboost', None):\n",
        "            model_xgboost = models['xgboost'][0]\n",
        "            p_xgboost = models['xgboost'][1]\n",
        "            x_val = data_dict['data']\n",
        "            df['prediction_xgb'] = model_xgboost.predict(x_val)\n",
        "        if models.get('lgb', None):\n",
        "            model_lgb = models['lgb'][0]\n",
        "            p_lgb = models['lgb'][1]\n",
        "            x_val = data_dict['data']\n",
        "            df['prediction_lgb'] = model_lgb.predict(x_val)\n",
        "            df = df[['id', 'prediction_lgb']]\n",
        "        pred_path = f'{get_data_path(root_dir)}/predictions/{era[0]}'\n",
        "        df.to_csv(f'{pred_path}.csv')\n",
        "\n",
        "\n",
        "def check_test_files(root_dir='./data'):\n",
        "    data_path = get_data_path(root_dir)\n",
        "    test_files_path = f'{data_path}/test_files'\n",
        "    # TODO Check to make sure all era's present\n",
        "    if os.path.isdir(test_files_path):\n",
        "        return test_files_path, True\n",
        "    else:\n",
        "        os.makedirs(test_files_path)\n",
        "        df = load_data(root_dir=root_dir, mode='test')\n",
        "        df['era'][df['era'] == 'eraX'] = 'era999'\n",
        "        for era in df['era'].unique():\n",
        "            path = f'{test_files_path}/{era}'\n",
        "            df[df['era'] == era].to_csv(f'{path}.csv')\n",
        "        return test_files_path, True\n",
        "\n",
        "\n",
        "def create_prediction_file(root_dir='./data', eras=None):\n",
        "    pred_path = f'{get_data_path(root_dir)}/predictions/'\n",
        "    files = os.listdir(pred_path)\n",
        "    files.sort()\n",
        "    if eras:\n",
        "        dfs = [pd.read_csv(f'{pred_path}{file}')\n",
        "               for file in files if file != 'predictions.csv' and file in eras]\n",
        "    else:\n",
        "        dfs = [pd.read_csv(f'{pred_path}{file}')\n",
        "               for file in files if file != 'predictions.csv']\n",
        "    df = pd.concat(dfs)\n",
        "    df = df[['id', 'prediction_lgb']]\n",
        "    df.columns = ['id', 'prediction']\n",
        "    df.to_csv(f'{pred_path}predictions.csv')\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import copy\n",
        "import os\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tqdm import tqdm\n",
        "from pytorch_lightning import loggers as pl_loggers\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "\n",
        "from data_loading.purged_group_time_series import PurgedGroupTimeSeriesSplit\n",
        "from data_loading.utils import load_data, preprocess_data, FinData, create_dataloaders, calc_data_mean, init_weights\n",
        "\n",
        "\n",
        "class ResNet(pl.LightningModule):\n",
        "    def __init__(self, input_size, output_size, params):\n",
        "        super(ResNet, self).__init__()\n",
        "        dim_1 = params['dim_1']\n",
        "        dim_2 = params['dim_2']\n",
        "        dim_3 = params['dim_3']\n",
        "        dim_4 = params['dim_4']\n",
        "        dim_5 = params['dim_5']\n",
        "        self.drop_prob = params['dropout']\n",
        "        self.drop = nn.Dropout(self.drop_prob)\n",
        "        self.lr = params['lr']\n",
        "        self.activation = params['activation']()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.loss = params['loss']()\n",
        "        if params['embedding']:\n",
        "            cat_dims = [5 for i in range(input_size)]\n",
        "            emb_dims = [(x, min(50, (x + 1) // 2)) for x in cat_dims]\n",
        "            self.embedding_layers = nn.ModuleList(\n",
        "                [nn.Embedding(x, y) for x, y in emb_dims]).to(self.device)\n",
        "            self.num_embeddings = sum([y for x, y in emb_dims])\n",
        "            if params.get('hidden_len', None):\n",
        "                self.input_size = self.num_embeddings + params['hidden_len']\n",
        "                self.d0 = nn.Linear(self.input_size, dim_1)\n",
        "                self.d1 = nn.Linear(dim_1 + self.input_size, dim_2)\n",
        "            else:\n",
        "                self.d0 = nn.Linear(self.num_embeddings, dim_1)\n",
        "                self.d1 = nn.Linear(dim_1 + self.num_embeddings, dim_2)\n",
        "\n",
        "        else:\n",
        "            self.d0 = nn.Linear(self.input_size, dim_1)\n",
        "            self.d1 = nn.Linear(dim_1+self.input_size, dim_2)\n",
        "\n",
        "        self.d2 = nn.Linear(dim_2 + dim_1, dim_3)\n",
        "        self.d3 = nn.Linear(dim_3 + dim_2, dim_4)\n",
        "        self.d4 = nn.Linear(dim_4 + dim_3, dim_5)\n",
        "        self.out = nn.Linear(dim_4+dim_5, output_size)\n",
        "\n",
        "        # Batch Norm\n",
        "        if params['embedding']:\n",
        "            if params['hidden_len']:\n",
        "                self.bn_hidden = nn.BatchNorm1d(params['hidden_len'])\n",
        "            self.bn0 = nn.BatchNorm1d(self.num_embeddings)\n",
        "        else:\n",
        "            self.bn0 = nn.BatchNorm1d(self.input_size)\n",
        "        self.bn1 = nn.BatchNorm1d(dim_1)\n",
        "        self.bn2 = nn.BatchNorm1d(dim_2)\n",
        "        self.bn3 = nn.BatchNorm1d(dim_3)\n",
        "        self.bn4 = nn.BatchNorm1d(dim_4)\n",
        "        self.bn5 = nn.BatchNorm1d(dim_5)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        if getattr(self, 'num_embeddings', None):\n",
        "            x = [emb_lay(x[:, i])\n",
        "                 for i, emb_lay in enumerate(self.embedding_layers)]\n",
        "            x = torch.cat(x, 1)\n",
        "        if hidden is not None:\n",
        "            hidden = self.bn_hidden(hidden)\n",
        "            x = torch.cat([x, hidden], 1)\n",
        "        # normalise inputs\n",
        "\n",
        "        # block 0\n",
        "        x1 = self.d0(x)\n",
        "        x1 = self.bn1(x1)\n",
        "        x1 = self.activation(x1)\n",
        "        x1 = self.drop(x1)\n",
        "\n",
        "        x = torch.cat([x, x1], 1)\n",
        "\n",
        "        # block 1\n",
        "        x2 = self.d1(x)\n",
        "        x2 = self.bn2(x2)\n",
        "        x2 = self.activation(x2)\n",
        "        x2 = self.drop(x2)\n",
        "\n",
        "        x = torch.cat([x1, x2], 1)\n",
        "\n",
        "        # block 2\n",
        "        x3 = self.d2(x)\n",
        "        x3 = self.bn3(x3)\n",
        "        x3 = self.activation(x3)\n",
        "        x3 = self.drop(x3)\n",
        "\n",
        "        x = torch.cat([x2, x3], 1)\n",
        "\n",
        "        # block 3\n",
        "        x4 = self.d3(x)\n",
        "        x4 = self.bn4(x4)\n",
        "        x4 = self.activation(x4)\n",
        "        x4 = self.drop(x4)\n",
        "\n",
        "        x = torch.cat([x3, x4], 1)\n",
        "\n",
        "        # block 4\n",
        "        x5 = self.d4(x)\n",
        "        x5 = self.bn5(x5)\n",
        "        x5 = self.activation(x5)\n",
        "        x5 = self.drop(x5)\n",
        "\n",
        "        x = torch.cat([x4, x5], 1)\n",
        "\n",
        "        out = self.out(x)\n",
        "        return out\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, hidden, y = batch['data'], batch.get(\n",
        "            'hidden', None), batch['target']\n",
        "        x = x.view(x.size(1), -1)\n",
        "        y = y.T\n",
        "        if hidden is not None:\n",
        "            hidden = hidden.view(hidden.size(1), -1)\n",
        "        logits = self(x, hidden)\n",
        "        loss = self.loss(input=logits, target=y)\n",
        "        mse = mean_squared_error(y_true=y.cpu().numpy(),\n",
        "                                 y_pred=logits.cpu().detach().numpy())\n",
        "        self.log('train_mse', mse, on_step=False,\n",
        "                 on_epoch=True, prog_bar=True)\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        return {'loss': loss}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, hidden, y = batch['data'], batch.get(\n",
        "            'hidden', None), batch['target']\n",
        "        x = x.view(x.size(1), -1)\n",
        "        y = y.T\n",
        "        if hidden is not None:\n",
        "            hidden = hidden.view(hidden.size(1), -1)\n",
        "        logits = self(x, hidden)\n",
        "        loss = self.loss(input=logits,\n",
        "                         target=y)\n",
        "        mse = mean_squared_error(y_true=y.cpu().numpy(),\n",
        "                                 y_pred=logits.cpu().detach().numpy())\n",
        "        return {'loss': loss, 'y': y, 'logits': logits, 'mse': mse}\n",
        "\n",
        "    def validation_epoch_end(self, val_step_outputs):\n",
        "        epoch_loss = torch.tensor([x['loss'] for x in val_step_outputs]).mean()\n",
        "        epoch_mse = torch.tensor([x['mse'] for x in val_step_outputs]).mean()\n",
        "        self.log('val_loss', epoch_loss, prog_bar=True)\n",
        "        self.log('val_mse', epoch_mse, prog_bar=True)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        return self.validation_step(batch, batch_idx)\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        epoch_loss = torch.tensor([x['loss'] for x in outputs]).mean()\n",
        "        epoch_mse = torch.tensor([x['mse'] for x in outputs]).mean()\n",
        "        self.log('test_loss', epoch_loss)\n",
        "        self.log('test_auc', epoch_mse)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(\n",
        "            self.parameters(), lr=self.lr)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, patience=5, factor=0.1, min_lr=1e-7, eps=1e-08\n",
        "        )\n",
        "        return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'val_mse'}\n",
        "\n",
        "\n",
        "def cross_val(p) -> object:\n",
        "    data_ = load_data(root_dir='./data/', mode='train')\n",
        "    data_, target_, features, date = preprocess_data(\n",
        "        data_, nn=True, action='multi')\n",
        "    input_size = data_.shape[-1]\n",
        "    output_size = target_.shape[-1]\n",
        "    gts = PurgedGroupTimeSeriesSplit(n_splits=5, group_gap=5)\n",
        "    models = []\n",
        "    tb_logger = pl_loggers.TensorBoardLogger('logs/multiclass_')\n",
        "    for i, (train_idx, val_idx) in enumerate(gts.split(data_, groups=date)):\n",
        "        idx = np.concatenate([train_idx, val_idx])\n",
        "        data = copy.deepcopy(data_[idx])\n",
        "        target = copy.deepcopy(target_[idx])\n",
        "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "            os.path.join('models/', 'multi_class_fold_{}'.format(i)), monitor='val_auc', save_top_k=1, period=10,\n",
        "            mode='max'\n",
        "        )\n",
        "        model = ResNet(input_size, output_size, p)\n",
        "        if p['activation'] == nn.ReLU:\n",
        "            model.apply(lambda m: init_weights(m, 'relu'))\n",
        "        elif p['activation'] == nn.LeakyReLU:\n",
        "            model.apply(lambda m: init_weights(m, 'leaky_relu'))\n",
        "        train_idx = [i for i in range(0, max(train_idx) + 1)]\n",
        "        val_idx = [i for i in range(len(train_idx), len(idx))]\n",
        "        data[train_idx] = calc_data_mean(\n",
        "            data[train_idx], './cache', train=True, mode='mean')\n",
        "        data[val_idx] = calc_data_mean(\n",
        "            data[val_idx], './cache', train=False, mode='mean')\n",
        "        dataset = FinData(data=data, target=target, date=date, multi=True)\n",
        "        dataloaders = create_dataloaders(\n",
        "            dataset, indexes={'train': train_idx, 'val': val_idx}, batch_size=p['batch_size'])\n",
        "        es = EarlyStopping(monitor='val_auc', patience=10,\n",
        "                           min_delta=0.0005, mode='max')\n",
        "        trainer = pl.Trainer(logger=tb_logger,\n",
        "                             max_epochs=100,\n",
        "                             gpus=1,\n",
        "                             callbacks=[checkpoint_callback, es],\n",
        "                             precision=16)\n",
        "        trainer.fit(\n",
        "            model, train_dataloader=dataloaders['train'], val_dataloaders=dataloaders['val'])\n",
        "        torch.save(model.state_dict(), f'models/fold_{i}_state_dict.pth')\n",
        "        models.append(model)\n",
        "    return models, features\n",
        "\n",
        "\n",
        "def fillna_npwhere(array, values):\n",
        "    if np.isnan(array.sum()):\n",
        "        array = np.nan_to_num(array) + np.isnan(array) * values\n",
        "    return array\n",
        "\n",
        "\n",
        "def test_model(models, features, cache_dir='cache'):\n",
        "    env = janestreet.make_env()\n",
        "    iter_test = env.iter_test()\n",
        "    if type(models) == list:\n",
        "        models = [model.eval() for model in models]\n",
        "    else:\n",
        "        models.eval()\n",
        "    f_mean = np.load(f'{cache_dir}/f_mean.npy')\n",
        "    for (test_df, sample_prediction_df) in tqdm(iter_test):\n",
        "        if test_df['weight'].item() > 0:\n",
        "            vals = torch.FloatTensor(\n",
        "                fillna_npwhere(test_df[features].values, f_mean))\n",
        "            if type(models) == list:\n",
        "                # calc mean of each models prediction of each response rather than mean of all predicted responses by each model\n",
        "                preds = [torch.sigmoid(model.forward(vals.view(1, -1))).detach().numpy()\n",
        "                         for model in models]\n",
        "                pred = np.mean(np.mean(preds, axis=0))\n",
        "            else:\n",
        "                pred = torch.sigmoid(models.forward(vals.view(1, -1))).item()\n",
        "            sample_prediction_df.action = np.where(\n",
        "                pred > 0.5, 1, 0).astype(int).item()\n",
        "        else:\n",
        "            sample_prediction_df.action = 0\n",
        "        env.predict(sample_prediction_df)\n",
        "\n",
        "\n",
        "def main():\n",
        "    p = {'dim_1': 167,\n",
        "         'dim_2': 454,\n",
        "         'dim_3': 371,\n",
        "         'dim_4': 369,\n",
        "         'dim_5': 155,\n",
        "         'activation': nn.LeakyReLU,\n",
        "         'dropout': 0.21062362698532755,\n",
        "         'lr': 0.0022252024054478523,\n",
        "         'label_smoothing': 0.05564974140461841,\n",
        "         'weight_decay': 0.04106097088288333,\n",
        "         'amsgrad': True,\n",
        "         'batch_size': 10072}\n",
        "    models, features = cross_val(p)\n",
        "    test_model(models, features)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "import datetime\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torch.utils.data.sampler import BatchSampler, SequentialSampler\n",
        "from tqdm import tqdm\n",
        "from purged_group_time_series import PurgedGroupTimeSeriesSplit\n",
        "from group_time_split import GroupTimeSeriesSplit\n",
        "from utils import load_data, preprocess_data, FinData\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_size, output_size, dims, batch_size, learning_rate=0.05, early_stopping=10,\n",
        "                 model_file='model.pth', fold=None):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.dims = dims\n",
        "        self.layer_list = nn.ModuleList()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.loss = nn.BCELoss()\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.early_stopping = early_stopping\n",
        "        self.model_file = self.create_model_file(model_file, fold)\n",
        "        self.batch_size = batch_size\n",
        "        self.train_log = pd.DataFrame({'auc': [0], 'loss': [0]})\n",
        "        self.val_log = pd.DataFrame({'auc': [0], 'loss': [0]})\n",
        "        for i in range(len(self.dims) + 1):\n",
        "            if i == 0:\n",
        "                self.layer_list.append(\n",
        "                    nn.Linear(self.input_size, self.dims[i]))\n",
        "                self.layer_list.append(nn.BatchNorm1d(self.dims[i]))\n",
        "            elif i == (len(self.dims)):\n",
        "                self.layer_list.append(\n",
        "                    nn.Linear(self.dims[i - 1], self.output_size))\n",
        "            else:\n",
        "                self.layer_list.append(\n",
        "                    nn.Linear(self.dims[i - 1], self.dims[i]))\n",
        "                self.layer_list.append(nn.BatchNorm1d(self.dims[i]))\n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            self.parameters(), lr=self.learning_rate)\n",
        "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer, patience=5, factor=0.1, min_lr=1e-7, eps=1e-08\n",
        "        )\n",
        "\n",
        "    def create_model_file(self, model_file, fold):\n",
        "        if not os.path.isdir('models'):\n",
        "            os.mkdir('models')\n",
        "        return 'models/nn_fold_{fold}_{model_file}'\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layer_list):\n",
        "            x = F.dropout(F.leaky_relu(self.layer_list[i](x)), p=0.2)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        with torch.set_grad_enabled(True):\n",
        "            x, y = batch['data'].to(\n",
        "                self.device), batch['target'].to(self.device)\n",
        "            x = x.reshape(x.size(1), -1)\n",
        "            y = y.reshape(-1, 1)\n",
        "            logits = self(x)\n",
        "            loss = self.loss(input=logits,\n",
        "                             target=y)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            return {'loss': loss, 'preds': logits, 'target': y}\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        with torch.set_grad_enabled(False):\n",
        "            x, y = batch['data'].to(\n",
        "                self.device), batch['target'].to(self.device)\n",
        "            x = x.reshape(x.size(1), -1)\n",
        "            y = y.reshape(-1, 1)\n",
        "            logits = self(x)\n",
        "            loss = self.loss(logits,\n",
        "                             target=y)\n",
        "            self.scheduler.step(loss)\n",
        "            return {'loss': loss, 'preds': logits, 'target': y}\n",
        "\n",
        "    def eval_step(self, data):\n",
        "        with torch.set_grad_enabled(False):\n",
        "            x, y = data['data'].to(self.device), data['target'].to(self.device)\n",
        "            x = x.reshape(x.size(1), -1)\n",
        "            y = y.reshape(-1, 1)\n",
        "            preds = self(x)\n",
        "            return y, preds\n",
        "\n",
        "    def batch_step_end_metrics(self, num_samples, batch_number, output, running_loss, running_metric):\n",
        "        running_loss += output['loss'].item()\n",
        "        running_metric += roc_auc_score(\n",
        "            output['target'].detach().cpu().numpy(),\n",
        "            output['preds'].detach().cpu().numpy())\n",
        "        return running_loss, running_metric\n",
        "\n",
        "    def epoch_end_metrics(self, outputs):\n",
        "        auc = torch.tensor([roc_auc_score(\n",
        "            out['target'].detach().cpu().numpy(),\n",
        "            out['preds'].detach().cpu().numpy()) for out in outputs])\n",
        "        loss = torch.stack([out['loss'] for out in outputs])\n",
        "        return torch.mean(auc), torch.mean(loss)\n",
        "\n",
        "    def log_results(self, phase, auc, loss):\n",
        "        if phase == 'train':\n",
        "            self.train_log = self.train_log.append(\n",
        "                {'auc': auc.item(), 'loss': loss.item()}, ignore_index=True)\n",
        "        if phase == 'val':\n",
        "            self.val_log = self.val_log.append(\n",
        "                {'auc': auc.item(), 'loss': loss.item()}, ignore_index=True)\n",
        "\n",
        "    def training_loop(self, epochs, dataloaders):\n",
        "        es_counter = 0\n",
        "        auc = {'train': -np.inf, 'eval': -np.inf}\n",
        "        loss = {'train': np.inf, 'eval': np.inf}\n",
        "        best_auc = -np.inf\n",
        "        for e, epoch in enumerate(range(epochs), 1):\n",
        "            for phase in ['train', 'val']:\n",
        "                bar = tqdm(dataloaders[phase])\n",
        "                outs = []\n",
        "                running_loss = 0.0\n",
        "                running_auc = 0.0\n",
        "                for b, batch in enumerate(bar, 1):\n",
        "                    bar.set_description(f'Epoch {epoch} {phase}'.ljust(20))\n",
        "                    if phase == 'train':\n",
        "                        self.train()\n",
        "                        out = self.training_step(batch)\n",
        "                    elif phase == 'val':\n",
        "                        self.eval()\n",
        "                        out = self.validation_step(batch)\n",
        "                    outs.append(out)\n",
        "                    num_samples = batch_size * b\n",
        "                    running_loss, running_auc = self.batch_step_end_metrics(\n",
        "                        num_samples, b, out, running_loss, running_auc)\n",
        "                    bar.set_postfix(loss=f'{running_loss / b:0.5f}',\n",
        "                                    auc=f'{running_auc / b:0.5f}')\n",
        "                auc[phase], loss[phase] = self.epoch_end_metrics(outs)\n",
        "                self.log_results(phase, auc[phase], loss[phase])\n",
        "                if phase == 'val' and auc['val'] > best_auc:\n",
        "                    # print('auc_val: ' + auc['val'], 'best_auc: ' + best_auc)\n",
        "                    best_auc = auc['val']\n",
        "                    best_model_weights = copy.deepcopy(self.state_dict())\n",
        "                    torch.save(best_model_weights, self.model_file)\n",
        "                    es_counter = 0\n",
        "            es_counter += 1\n",
        "            if es_counter > self.early_stopping:\n",
        "                print(\n",
        "                    f'Early Stopping limit reached. Best Model saved to {self.model_file}')\n",
        "                print(f'Best Metric achieved: {best_auc}')\n",
        "                break\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        nn.init.xavier_normal_(m.weight, nn.init.calculate_gain('leaky_relu'))\n",
        "        m.bias.data.fill_(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = load_data('data/', mode='train', overide='filtered_train.csv')\n",
        "data, target, features, date = preprocess_data(data, scale=True)\n",
        "# %%\n",
        "dataset = FinData(data=data, target=target, date=date)\n",
        "# %%\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "dims = [384, 896, 896, 394]\n",
        "batch_size = 500\n",
        "epochs = 100\n",
        "gts = GroupTimeSeriesSplit(n_splits=5)\n",
        "#gts = PurgedGroupTimeSeriesSplit(n_splits=5, group_gap=31)\n",
        "train_model = True\n",
        "eval_model = False\n",
        "for i, (train_idx, val_idx) in enumerate(gts.split(data, groups=date)):\n",
        "    if train_model:\n",
        "        model = Classifier(input_size=len(features), output_size=1,\n",
        "                           dims=dims, batch_size=batch_size,\n",
        "                           model_file=f'nn_model_fold_{i}.pth').to(device=device)\n",
        "\n",
        "        # model.apply(init_weights)\n",
        "        train_set, val_set = Subset(\n",
        "            dataset, train_idx), Subset(dataset, val_idx)\n",
        "        train_sampler = BatchSampler(SequentialSampler(\n",
        "            train_set), batch_size=batch_size, drop_last=False)\n",
        "        val_sampler = BatchSampler(SequentialSampler(\n",
        "            val_set), batch_size=batch_size, drop_last=False)\n",
        "        dataloaders = {'train': DataLoader(dataset, sampler=train_sampler, num_workers=6),\n",
        "                       'val': DataLoader(dataset, sampler=val_sampler, num_workers=6)}\n",
        "        model.training_loop(epochs=epochs, dataloaders=dataloaders)\n",
        "        model.train_log.to_csv(\n",
        "            f'logs/train_fold_{i}_{str(datetime.datetime.now())}.csv')\n",
        "        model.val_log.to_csv(\n",
        "            f'logs/val_fold_{i}_{str(datetime.datetime.now())}.csv')\n",
        "    if eval_model:\n",
        "        model = Classifier(input_size=len(features), output_size=1,\n",
        "                           dims=dims, batch_size=batch_size,\n",
        "                           model_file=f'nn_model_fold_{i}_{datetime.datetime.now()}.pth').to(device=device)\n",
        "        checkpoint = torch.load(model.model_file)\n",
        "        model.load_state_dict(checkpoint)\n",
        "        model.eval()\n",
        "        val_set = Subset(dataset, val_idx)\n",
        "        val_sampler = BatchSampler(SequentialSampler(\n",
        "            val_set), batch_size=batch_size, drop_last=False)\n",
        "        val_loader = DataLoader(dataset, sampler=val_sampler, num_workers=6)\n",
        "        bar = tqdm(val_loader)\n",
        "        all_preds = []\n",
        "        all_y_true = []\n",
        "        for b, batch in enumerate(bar, 1):\n",
        "            bar.set_description(f'Evaluating Model')\n",
        "            y_true, preds = model.eval_step(batch)\n",
        "            all_y_true.append(y_true.cpu().numpy())\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "        all_preds = np.concatenate(all_preds, axis=0)\n",
        "        all_y_true = np.concatenate(all_y_true, axis=0)\n",
        "        fpr, tpr, _ = roc_curve(all_y_true, all_preds)\n",
        "        plt.plot(fpr, tpr, label='nn')\n",
        "        plt.savefig(\n",
        "            f'plots/val_fold_{i}_roc_curve.png')\n",
        "        plt.close()\n",
        "# %%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from numpy.lib.function_base import _parse_input_dimensions\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, plot_roc_curve\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch._C import dtype\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from pytorch_lightning.core.lightning import LightningModule\n",
        "from pytorch_lightning import Trainer, trainer\n",
        "import datatable as dt\n",
        "import pandas as pd\n",
        "from torch.nn.modules.loss import BCEWithLogitsLoss\n",
        "from group_time_split import GroupTimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, dataloader\n",
        "from torch.utils.data.sampler import BatchSampler, SequentialSampler\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_size, output_size, dims, batch_size, learning_rate=0.05, early_stopping=10,\n",
        "                 model_file='model.pth'):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.dims = dims\n",
        "        self.layer_list = nn.ModuleList()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.loss = nn.BCELoss()\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.early_stopping = early_stopping\n",
        "        self.model_file = model_file\n",
        "        self.batch_size = batch_size\n",
        "        self.train_log = pd.DataFrame({'auc': [0], 'loss': [0]})\n",
        "        self.val_log = pd.DataFrame({'auc': [0], 'loss': [0]})\n",
        "        for i in range(len(self.dims)+1):\n",
        "            if i == 0:\n",
        "                self.layer_list.append(\n",
        "                    nn.Linear(self.input_size, self.dims[i]))\n",
        "                self.layer_list.append(nn.BatchNorm1d(self.dims[i]))\n",
        "            elif i == (len(self.dims)):\n",
        "                self.layer_list.append(\n",
        "                    nn.Linear(self.dims[i-1], self.output_size))\n",
        "            else:\n",
        "                self.layer_list.append(nn.Linear(self.dims[i-1], self.dims[i]))\n",
        "                self.layer_list.append(nn.BatchNorm1d(self.dims[i]))\n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            self.parameters(), lr=self.learning_rate)\n",
        "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer, patience=5, factor=0.1, min_lr=1e-7, eps=1e-08\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layer_list):\n",
        "            x = F.dropout(F.leaky_relu(self.layer_list[i](x)), p=0.2)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        with torch.set_grad_enabled(True):\n",
        "            x, y = batch['data'].to(\n",
        "                self.device), batch['target'].to(self.device)\n",
        "            x = x.reshape(x.size(1), -1)\n",
        "            y = y.reshape(-1, 1)\n",
        "            logits = self(x)\n",
        "            loss = self.loss(input=logits,\n",
        "                             target=y)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            return {'loss': loss, 'preds': logits, 'target': y}\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        with torch.set_grad_enabled(False):\n",
        "            x, y = batch['data'].to(\n",
        "                self.device), batch['target'].to(self.device)\n",
        "            x = x.reshape(x.size(1), -1)\n",
        "            y = y.reshape(-1, 1)\n",
        "            logits = self(x)\n",
        "            loss = self.loss(logits,\n",
        "                             target=y)\n",
        "            self.scheduler.step(loss)\n",
        "            return {'loss': loss, 'preds': logits, 'target': y}\n",
        "\n",
        "    def eval_step(self, data):\n",
        "        with torch.set_grad_enabled(False):\n",
        "            x, y = data['data'].to(self.device), data['target'].to(self.device)\n",
        "            x = x.reshape(x.size(1), -1)\n",
        "            y = y.reshape(-1, 1)\n",
        "            preds = self(x)\n",
        "            return y, preds\n",
        "\n",
        "    def batch_step_end_metrics(self, num_samples, batch_number, output, running_loss, running_metric):\n",
        "        running_loss += output['loss'].item()\n",
        "        running_metric += roc_auc_score(\n",
        "            output['target'].detach().cpu().numpy(),\n",
        "            output['preds'].detach().cpu().numpy())\n",
        "        return running_loss, running_metric\n",
        "\n",
        "    def epoch_end_metrics(self, outputs):\n",
        "        auc = torch.tensor([roc_auc_score(\n",
        "            out['target'].detach().cpu().numpy(),\n",
        "            out['preds'].detach().cpu().numpy()) for out in outputs])\n",
        "        loss = torch.stack([out['loss'] for out in outputs])\n",
        "        return torch.mean(auc), torch.mean(loss)\n",
        "\n",
        "    def log_results(self, phase, auc, loss):\n",
        "        if phase == 'train':\n",
        "            self.train_log = self.train_log.append(\n",
        "                {'auc': auc.item(), 'loss': loss.item()}, ignore_index=True)\n",
        "        if phase == 'val':\n",
        "            self.val_log = self.val_log.append(\n",
        "                {'auc': auc.item(), 'loss': loss.item()}, ignore_index=True)\n",
        "\n",
        "    def training_loop(self, epochs, dataloaders):\n",
        "        es_counter = 0\n",
        "        auc = {'train': -np.inf, 'eval': -np.inf}\n",
        "        loss = {'train': np.inf, 'eval': np.inf}\n",
        "        best_auc = -np.inf\n",
        "        for e, epoch in enumerate(range(epochs), 1):\n",
        "            for phase in ['train', 'val']:\n",
        "                bar = tqdm(dataloaders[phase])\n",
        "                outs = []\n",
        "                running_loss = 0.0\n",
        "                running_auc = 0.0\n",
        "                for b, batch in enumerate(bar, 1):\n",
        "                    bar.set_description(f'Epoch {epoch} {phase}'.ljust(20))\n",
        "                    if phase == 'train':\n",
        "                        self.train()\n",
        "                        out = self.training_step(batch)\n",
        "                    elif phase == 'val':\n",
        "                        self.eval()\n",
        "                        out = self.validation_step(batch)\n",
        "                    outs.append(out)\n",
        "                    num_samples = batch_size*b\n",
        "                    running_loss, running_auc = self.batch_step_end_metrics(\n",
        "                        num_samples, b, out, running_loss, running_auc)\n",
        "                    bar.set_postfix(loss=f'{running_loss/b:0.5f}',\n",
        "                                    auc=f'{running_auc/b:0.5f}')\n",
        "                auc[phase], loss[phase] = self.epoch_end_metrics(outs)\n",
        "                self.log_results(phase, auc[phase], loss[phase])\n",
        "                if phase == 'val' and auc['val'] > best_auc:\n",
        "                    # print('auc_val: ' + auc['val'], 'best_auc: ' + best_auc)\n",
        "                    best_auc = auc['val']\n",
        "                    best_model_weights = copy.deepcopy(self.state_dict())\n",
        "                    torch.save(best_model_weights, self.model_file)\n",
        "                    es_counter = 0\n",
        "            es_counter += 1\n",
        "            if es_counter > self.early_stopping:\n",
        "                print(\n",
        "                    f'Early Stopping limit reached. Best Model saved to {self.model_file}')\n",
        "                print(f'Best Metric achieved: {best_auc}')\n",
        "                break\n",
        "\n",
        "\n",
        "class FinData(Dataset):\n",
        "    def __init__(self, data, target, mode='train', transform=None, cache_dir=None):\n",
        "        self.data = data\n",
        "        self.target = target\n",
        "        self.mode = mode\n",
        "        self.transform = transform\n",
        "        self.cache_dir = cache_dir\n",
        "        self.date = date\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index.to_list()\n",
        "        if self.transform:\n",
        "            return self.transform(self.data.iloc[index].values)\n",
        "        else:\n",
        "            if type(index) is list:\n",
        "                sample = {\n",
        "                    'target': torch.Tensor(self.target.iloc[index].values),\n",
        "                    'data': torch.FloatTensor(self.data[index]),\n",
        "                    'date': torch.Tensor(self.date.iloc[index].values)\n",
        "                }\n",
        "                return sample\n",
        "            else:\n",
        "                sample = {\n",
        "                    'target': torch.Tensor(self.target.iloc[index]),\n",
        "                    'data': torch.FloatTensor(self.data[index]),\n",
        "                    'date': torch.Tensor(self.date.iloc[index])\n",
        "                }\n",
        "                return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        nn.init.xavier_normal_(m.weight, nn.init.calculate_gain('leaky_relu'))\n",
        "        m.bias.data.fill_(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\r\n",
        "\r\n",
        "def load_data(root_dir, mode, overide=None):\r\n",
        "    if overide:\r\n",
        "        data = dt.fread(overide).to_pandas()\r\n",
        "    elif mode == 'train':\r\n",
        "        data = dt.fread(root_dir+'train.csv').to_pandas()\r\n",
        "    elif mode == 'test':\r\n",
        "        data = dt.fread(root_dir+'example_test.csv').to_pandas()\r\n",
        "    elif mode == 'sub':\r\n",
        "        data = dt.fread(root_dir+'example_sample_submission.csv').to_pandas()\r\n",
        "    return data\r\n",
        "\r\n",
        "\r\n",
        "def preprocess_data(data):\r\n",
        "    # data = data.query('weight > 0').reset_index(drop=True)\r\n",
        "    data['action'] = ((data['resp'].values) > 0).astype('float32')\r\n",
        "    features = [\r\n",
        "        col for col in data.columns if 'feature' in col and col != 'feature_0']+['weight']\r\n",
        "    for col in features:\r\n",
        "        data[col].fillna(data[col].mean(), inplace=True)\r\n",
        "    target = data['action']\r\n",
        "    date = data['date']\r\n",
        "    data = data[features]\r\n",
        "    scaler = StandardScaler()\r\n",
        "    data = scaler.fit_transform(data)\r\n",
        "\r\n",
        "    return data, target, features, date\r\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = load_data('data/', mode='train', overide='filtered_train.csv')\n",
        "data, target, features, date = preprocess_data(data)\n",
        "# %%\n",
        "dataset = FinData(data, target, features)\n",
        "# %%\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "dims = [384, 896, 896, 394]\n",
        "batch_size = 1000\n",
        "epochs = 100\n",
        "gts = GroupTimeSeriesSplit()\n",
        "train_model = False\n",
        "eval_model = True\n",
        "for i, (train_idx, val_idx) in enumerate(gts.split(data, groups=date)):\n",
        "    if train_model:\n",
        "        model = Classifier(input_size=len(features), output_size=1,\n",
        "                           dims=dims, batch_size=batch_size,\n",
        "                           model_file=f'nn_model_fold_{i}.pth').to(device=device)\n",
        "\n",
        "        # model.apply(init_weights)\n",
        "        train_set, val_set = Subset(\n",
        "            dataset, train_idx), Subset(dataset, val_idx)\n",
        "        train_sampler = BatchSampler(SequentialSampler(\n",
        "            train_set), batch_size=batch_size, drop_last=False)\n",
        "        val_sampler = BatchSampler(SequentialSampler(\n",
        "            val_set), batch_size=batch_size, drop_last=False)\n",
        "        dataloaders = {'train': DataLoader(dataset, sampler=train_sampler, num_workers=6),\n",
        "                       'val': DataLoader(dataset, sampler=val_sampler, num_workers=6)}\n",
        "        model.training_loop(epochs=epochs, dataloaders=dataloaders)\n",
        "        model.train_log.to_csv(\n",
        "            f'logs/train_fold_{i}_{str(datetime.datetime.now())}.csv')\n",
        "        model.val_log.to_csv(\n",
        "            f'logs/val_fold_{i}_{str(datetime.datetime.now())}.csv')\n",
        "    if eval_model:\n",
        "        model = Classifier(input_size=len(features), output_size=1,\n",
        "                           dims=dims, batch_size=batch_size,\n",
        "                           model_file=f'nn_model_fold_{i}.pth').to(device=device)\n",
        "        checkpoint = torch.load(model.model_file)\n",
        "        model.load_state_dict(checkpoint)\n",
        "        model.eval()\n",
        "        val_set = Subset(dataset, val_idx)\n",
        "        val_sampler = BatchSampler(SequentialSampler(\n",
        "            val_set), batch_size=batch_size, drop_last=False)\n",
        "        val_loader = DataLoader(dataset, sampler=val_sampler, num_workers=6)\n",
        "        bar = tqdm(val_loader)\n",
        "        all_preds = []\n",
        "        all_y_true = []\n",
        "        for b, batch in enumerate(bar, 1):\n",
        "            bar.set_description(f'Evaluating Model')\n",
        "            y_true, preds = model.eval_step(batch)\n",
        "            all_y_true.append(y_true.cpu().numpy())\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "        all_preds = np.concatenate(all_preds, axis=0)\n",
        "        all_y_true = np.concatenate(all_y_true, axis=0)\n",
        "        fpr, tpr, _ = roc_curve(all_y_true, all_preds)\n",
        "        plt.plot(fpr, tpr, label='nn')\n",
        "        plt.savefig(\n",
        "            f'plots/val_fold_{i}_roc_curve.png')\n",
        "# %%\n",
        "val_set[:]['target']\n",
        "# %%\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from numba import njit\n",
        "from pytorch_lightning import Callback\n",
        "from pytorch_lightning import loggers as pl_loggers\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch.nn.modules import linear\n",
        "from torch.nn.modules.batchnorm import BatchNorm1d\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "class MetricsCallback(Callback):\n",
        "    \"\"\"PyTorch Lightning metric callback.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.metrics = []\n",
        "\n",
        "    def on_validation_end(self, trainer, pl_module):\n",
        "        self.metrics.append(trainer.callback_metrics)\n",
        "\n",
        "\n",
        "class Classifier(pl.LightningModule):\n",
        "    def __init__(self, input_size, output_size, params=None,\n",
        "                 model_path='models/'):\n",
        "        super(Classifier, self).__init__()\n",
        "        dim_1 = params['dim_1']\n",
        "        dim_2 = params['dim_2']\n",
        "        dim_3 = params['dim_3']\n",
        "        dim_4 = params['dim_4']\n",
        "        dim_5 = params['dim_5']\n",
        "        self.dropout_prob = params['dropout']\n",
        "        self.lr = params['lr']\n",
        "        self.activation = params['activation']\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.loss = params['loss']\n",
        "        self.weight_decay = params['weight_decay']\n",
        "        self.amsgrad = params['amsgrad']\n",
        "        self.label_smoothing = params['label_smoothing']\n",
        "        self.train_log = pd.DataFrame({'auc': [0], 'loss': [0]})\n",
        "        self.val_log = pd.DataFrame({'auc': [0], 'loss': [0]})\n",
        "        self.model_path = model_path\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.BatchNorm1d(input_size),\n",
        "            nn.Linear(input_size, dim_1, bias=False),\n",
        "            nn.BatchNorm1d(dim_1),\n",
        "            self.activation(),\n",
        "            nn.Dropout(p=self.dropout_prob),\n",
        "            nn.Linear(dim_1, dim_2, bias=False),\n",
        "            nn.BatchNorm1d(dim_2),\n",
        "            self.activation(),\n",
        "            nn.Dropout(p=self.dropout_prob),\n",
        "            nn.Linear(dim_2, dim_3, bias=False),\n",
        "            nn.BatchNorm1d(dim_3),\n",
        "            self.activation(),\n",
        "            nn.Dropout(p=self.dropout_prob),\n",
        "            nn.Linear(dim_3, dim_4, bias=False),\n",
        "            nn.BatchNorm1d(dim_4),\n",
        "            self.activation(),\n",
        "            nn.Dropout(p=self.dropout_prob),\n",
        "            nn.Linear(dim_4, dim_5, bias=False),\n",
        "            nn.BatchNorm1d(dim_5),\n",
        "            self.activation(),\n",
        "            nn.Dropout(p=self.dropout_prob),\n",
        "            nn.Linear(dim_5, self.output_size, bias=False)\n",
        "        )\n",
        "        self.encoder_1l = nn.Sequential(\n",
        "            nn.BatchNorm1d(input_size),\n",
        "            nn.Linear(input_size, dim_1, bias=False),\n",
        "            nn.BatchNorm1d(dim_1),\n",
        "            self.activation(),\n",
        "            nn.Dropout(p=self.dropout_prob),\n",
        "            nn.Linear(dim_1, self.output_size, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.encoder_1l(x)\n",
        "        return out\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch['data'], batch['target']\n",
        "        x = x.view(x.size(1), -1)\n",
        "        y = y.T\n",
        "        logits = self(x)\n",
        "        loss = self.loss(input=logits, target=y)\n",
        "        mse = mean_squared_error(y_true=y.cpu().numpy(),\n",
        "                                 y_pred=logits.cpu().detach().numpy())\n",
        "        self.log('train_mse', mse, on_step=False,\n",
        "                 on_epoch=True, prog_bar=True)\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        return {'loss': loss}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch['data'], batch['target']\n",
        "        x = x.view(x.size(1), -1)\n",
        "        y = y.T\n",
        "        logits = self(x)\n",
        "        loss = self.loss(input=logits,\n",
        "                         target=y)\n",
        "        mse = mean_squared_error(y_true=y.cpu().numpy(),\n",
        "                                 y_pred=logits.cpu().detach().numpy())\n",
        "        return {'loss': loss, 'y': y, 'logits': logits, 'mse': mse}\n",
        "\n",
        "    def validation_epoch_end(self, val_step_outputs):\n",
        "        epoch_loss = torch.tensor([x['loss'] for x in val_step_outputs]).mean()\n",
        "        epoch_mse = torch.tensor([x['mse'] for x in val_step_outputs]).mean()\n",
        "        self.log('val_loss', epoch_loss, prog_bar=True)\n",
        "        self.log('val_mse', epoch_mse, prog_bar=True)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        return self.validation_step(batch, batch_idx)\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        epoch_loss = torch.tensor([x['loss'] for x in outputs]).mean()\n",
        "        epoch_auc = torch.tensor([x['auc'] for x in outputs]).mean()\n",
        "        self.log('test_loss', epoch_loss)\n",
        "        self.log('test_auc', epoch_auc)\n",
        "\n",
        "    def predict(self, batch):\n",
        "        self.eval()\n",
        "        x, y = batch['data'], batch['target']\n",
        "        x = x.view(x.size(1), -1)\n",
        "        x = self(x)\n",
        "        return torch.sigmoid(x.view(-1))\n",
        "\n",
        "    def prediction_loop(self, dataloader, return_tensor=True):\n",
        "        bar = tqdm(dataloader)\n",
        "        preds = []\n",
        "        for batch in bar:\n",
        "            preds.append(self.predict(batch))\n",
        "        if return_tensor:\n",
        "            return torch.cat(preds, dim=0)\n",
        "        else:\n",
        "            return preds\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # weight_decay = self.weight_decay,\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr,\n",
        "                                     amsgrad=self.amsgrad)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, patience=5, factor=0.1, min_lr=1e-7, eps=1e-08\n",
        "        )\n",
        "        return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'val_mse'}\n",
        "\n",
        "\n",
        "def init_weights(m, func):\n",
        "    if type(m) == nn.Linear:\n",
        "        nn.init.xavier_normal_(m.weight, nn.init.calculate_gain(func))\n",
        "        # m.bias.data.fill_(1)\n",
        "\n",
        "\n",
        "def train_cross_val(p):\n",
        "    data_ = load_data(root_dir='./data/', mode='train')\n",
        "    data_, target_, features, date = preprocess_data(data_, nn=True)\n",
        "\n",
        "    gts = PurgedGroupTimeSeriesSplit(n_splits=5, group_gap=5)\n",
        "\n",
        "    input_size = data_.shape[-1]\n",
        "    output_size = 1\n",
        "    tb_logger = pl_loggers.TensorBoardLogger('logs/')\n",
        "    models = []\n",
        "    for i, (train_idx, val_idx) in enumerate(gts.split(data_, groups=date)):\n",
        "        idx = np.concatenate([train_idx, val_idx])\n",
        "        data = copy.deepcopy(data_[idx])\n",
        "        target = copy.deepcopy(target_[idx])\n",
        "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "            os.path.join('models/', \"fold_{}\".format(i)), monitor=\"val_auc\", mode='max', save_top_k=1, period=10)\n",
        "        model = Classifier(input_size=input_size,\n",
        "                           output_size=output_size, params=p)\n",
        "        if p['activation'] == nn.ReLU:\n",
        "            model.apply(lambda m: init_weights(m, 'relu'))\n",
        "        elif p['activation'] == nn.LeakyReLU:\n",
        "            model.apply(lambda m: init_weights(m, 'leaky_relu'))\n",
        "        train_idx = [i for i in range(0, max(train_idx) + 1)]\n",
        "        val_idx = [i for i in range(len(train_idx), len(idx))]\n",
        "        data[train_idx] = calc_data_mean(\n",
        "            data[train_idx], './cache', train=True, mode='mean')\n",
        "        data[val_idx] = calc_data_mean(\n",
        "            data[val_idx], './cache', train=False, mode='mean')\n",
        "        dataset = FinData(data=data, target=target, date=date)\n",
        "        dataloaders = create_dataloaders(\n",
        "            dataset, indexes={'train': train_idx, 'val': val_idx}, batch_size=p['batch_size'])\n",
        "        es = EarlyStopping(monitor='val_auc', patience=10,\n",
        "                           min_delta=0.0005, mode='max')\n",
        "        trainer = pl.Trainer(logger=tb_logger,\n",
        "                             max_epochs=500,\n",
        "                             gpus=1,\n",
        "                             callbacks=[checkpoint_callback, es],\n",
        "                             precision=16)\n",
        "        trainer.fit(\n",
        "            model, train_dataloader=dataloaders['train'], val_dataloaders=dataloaders['val'])\n",
        "        torch.save(model.state_dict(), f'models/fold_{i}_state_dict.pth')\n",
        "        models.append(model)\n",
        "    return models, features\n",
        "\n",
        "\n",
        "def final_train(p, load=False):\n",
        "    data_ = load_data(root_dir='./data/', mode='train')\n",
        "    data, target, features, date = preprocess_data(data_, nn=True)\n",
        "    input_size = data.shape[-1]\n",
        "    output_size = 1\n",
        "    train_idx, val_idx = date[date <= 450].index.values.tolist(\n",
        "    ), date[date > 450].index.values.tolist()\n",
        "    data[train_idx] = calc_data_mean(data[train_idx], './cache', train=True)\n",
        "    data[val_idx] = calc_data_mean(data[val_idx], './cache', train=False)\n",
        "    checkpoint_callback = pl.callbacks.ModelCheckpoint(filepath='models/full_train',\n",
        "                                                       monitor=\"val_auc\", mode='max', save_top_k=1, period=10)\n",
        "    model = Classifier(input_size=input_size,\n",
        "                       output_size=output_size, params=p)\n",
        "    if p['activation'] == nn.ReLU:\n",
        "        model.apply(lambda m: init_weights(m, 'relu'))\n",
        "    elif p['activation'] == nn.LeakyReLU:\n",
        "        model.apply(lambda m: init_weights(m, 'leaky_relu'))\n",
        "    dataset = FinData(data, target, date)\n",
        "    dataloaders = create_dataloaders(\n",
        "        dataset, indexes={'train': train_idx, 'val': val_idx}, batch_size=p['batch_size'])\n",
        "    es = EarlyStopping(monitor='val_auc', patience=10,\n",
        "                       min_delta=0.0005, mode='max')\n",
        "    trainer = pl.Trainer(max_epochs=500,\n",
        "                         gpus=1,\n",
        "                         callbacks=[checkpoint_callback, es],\n",
        "                         precision=16)\n",
        "    trainer.fit(\n",
        "        model, train_dataloader=dataloaders['train'], val_dataloaders=dataloaders['val'])\n",
        "    torch.save(model.state_dict(), 'models/final_train.pth')\n",
        "    return model, features\n",
        "\n",
        "\n",
        "def fillna_npwhere(array, values):\n",
        "    if np.isnan(array.sum()):\n",
        "        array = np.nan_to_num(array) + np.isnan(array) * values\n",
        "    return array\n",
        "\n",
        "\n",
        "def test_model(models, features, cache_dir='cache'):\n",
        "    env = janestreet.make_env()\n",
        "    iter_test = env.iter_test()\n",
        "    if type(models) == list:\n",
        "        models = [model.eval() for model in models]\n",
        "    else:\n",
        "        models.eval()\n",
        "    f_mean = np.load(f'{cache_dir}/f_mean.npy')\n",
        "    for (test_df, sample_prediction_df) in tqdm(iter_test):\n",
        "        if test_df['weight'].item() > 0:\n",
        "            vals = torch.FloatTensor(\n",
        "                fillna_npwhere(test_df[features].values, f_mean))\n",
        "            if type(models) == list:\n",
        "                preds = [torch.sigmoid(model.forward(vals.view(1, -1))).item()\n",
        "                         for model in models]\n",
        "                pred = np.median(preds)\n",
        "            else:\n",
        "                pred = torch.sigmoid(models.forward(vals.view(1, -1))).item()\n",
        "            sample_prediction_df.action = np.where(\n",
        "                pred > 0.5, 1, 0).astype(int).item()\n",
        "        else:\n",
        "            sample_prediction_df.action = 0\n",
        "        env.predict(sample_prediction_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "import gc\n",
        "\n",
        "import datatable as dt\n",
        "import joblib\n",
        "import lightgbm as lgb\n",
        "import neptune\n",
        "import neptunecontrib.monitoring.optuna as opt_utils\n",
        "import numpy as np\n",
        "import optuna\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from data_loading import utils\n",
        "from data_loading import purged_group_time_series as pgs\n",
        "\n",
        "\n",
        "def optimize(trial: optuna.trial.Trial, datasets):\n",
        "    p = {'learning_rate': trial.suggest_uniform('learning_rate', 1e-4, 1e-1),\n",
        "         'max_depth': trial.suggest_int('max_depth', 5, 30),\n",
        "         'max_leaves': trial.suggest_int('max_leaves', 5, 50),\n",
        "         'subsample': trial.suggest_uniform('subsample', 0.3, 1.0),\n",
        "         'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.3, 1.0),\n",
        "         'min_child_weight': trial.suggest_int('min_child_weight', 5, 100),\n",
        "         'lambda': trial.suggest_uniform('lambda', 0.05, 0.2),\n",
        "         'alpha': trial.suggest_uniform('alpha', 0.05, 0.2),\n",
        "         'objective': 'regression',\n",
        "         'booster': 'gbtree',\n",
        "         'tree_method': 'gpu_hist',\n",
        "         'verbosity': 1,\n",
        "         'n_jobs': 10,\n",
        "         'eval_metric': 'mse'}\n",
        "    print('Choosing parameters:', p)\n",
        "    scores = []\n",
        "    sizes = []\n",
        "    # gts = GroupTimeSeriesSplit()\n",
        "    gts = pgs.PurgedGroupTimeSeriesSplit(n_splits=5, group_gap=31)\n",
        "    for i, (tr_idx, val_idx) in enumerate(gts.split(datasets['data'], groups=datasets['era'])):\n",
        "        sizes.append(len(tr_idx))\n",
        "        x_tr, x_val = datasets['data'][tr_idx], datasets['data'][val_idx]\n",
        "        y_tr, y_val = datasets['target'][tr_idx], datasets['target'][val_idx]\n",
        "        d_tr = xgb.DMatrix(x_tr, y_tr)\n",
        "        d_val = xgb.DMatrix(x_val, y_val)\n",
        "        clf = xgb.train(p, d_tr, 1000, [\n",
        "            (d_val, 'eval')], early_stopping_rounds=50, verbose_eval=True)\n",
        "        val_pred = clf.predict(d_val)\n",
        "        score = mean_squared_error(y_val, val_pred)\n",
        "        scores.append(score)\n",
        "        del clf, val_pred, d_tr, d_val, x_tr, x_val, y_tr, y_val, score\n",
        "        rubbish = gc.collect()\n",
        "    print(scores)\n",
        "    avg_score = utils.weighted_mean(scores, sizes)\n",
        "    print('Avg Score:', avg_score)\n",
        "    return avg_score\n",
        "\n",
        "\n",
        "def loptimize(trial, datasets):\n",
        "    p = {'learning_rate': trial.suggest_uniform('learning_rate', 1e-4, 1e-1),\n",
        "         'max_leaves': trial.suggest_int('max_leaves', 5, 100),\n",
        "         'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.3, 0.99),\n",
        "         'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),\n",
        "         'feature_fraction': trial.suggest_uniform('feature_fraction', 0.3, 0.99),\n",
        "         'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 50, 1000),\n",
        "         'lambda_l1': trial.suggest_uniform('lambda_l1', 0.005, 0.05),\n",
        "         'lambda_l2': trial.suggest_uniform('lambda_l2', 0.005, 0.05),\n",
        "         'boosting': trial.suggest_categorical('boosting', ['gbdt', 'goss', 'rf']),\n",
        "         'objective': 'binary',\n",
        "         'verbose': 1,\n",
        "         'n_jobs': 10,\n",
        "         'metric': 'auc'}\n",
        "    if p['boosting'] == 'goss':\n",
        "        p['bagging_freq'] = 0\n",
        "        p['bagging_fraction'] = 1.0\n",
        "    scores = []\n",
        "    sizes = []\n",
        "    # gts = GroupTimeSeriesSplit()\n",
        "    gts = pgs.PurgedGroupTimeSeriesSplit(n_splits=5, group_gap=31)\n",
        "    for i, (tr_idx, val_idx) in enumerate(gts.split(datasets['data'], groups=datasets['era'])):\n",
        "        sizes.append(len(tr_idx))\n",
        "        x_tr, x_val = datasets['data'][tr_idx], datasets['data'][val_idx]\n",
        "        y_tr, y_val = datasets['target'][tr_idx], datasets['target'][val_idx]\n",
        "        train = lgb.Dataset(x_tr, label=y_tr)\n",
        "        val = lgb.Dataset(x_val, label=y_val)\n",
        "        clf = lgb.train(p, train, 1000, valid_sets=[\n",
        "            val], early_stopping_rounds=50, verbose_eval=True)\n",
        "        preds = clf.predict(x_val)\n",
        "        score = mean_squared_error(y_val, preds)\n",
        "        scores.append(score)\n",
        "        del clf, preds, train, val, x_tr, x_val, y_tr, y_val, score\n",
        "        rubbish = gc.collect()\n",
        "    print(scores)\n",
        "    avg_score = utils.weighted_mean(scores, sizes)\n",
        "    print('Avg Score:', avg_score)\n",
        "    return avg_score\n",
        "\n",
        "\n",
        "def main():\n",
        "    api_token = 'eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzMGQ1MWZiNy1iYjNlLTQ3NDctOTE4OS1lNzhlNmVlYmUwMzYifQ=='\n",
        "    neptune.init(api_token=api_token,\n",
        "                 project_qualified_name='kramerji/Numerai')\n",
        "    data = utils.load_data('data/', mode='train')\n",
        "    data, target, features, era = utils.preprocess_data(data, nn=True)\n",
        "    datasets = {'data': data, 'target': target,\n",
        "                'features': features, 'era': era}\n",
        "    print('creating XGBoost Trials')\n",
        "    xgb_exp = neptune.create_experiment('XGBoost_HPO')\n",
        "    xgb_neptune_callback = opt_utils.NeptuneCallback(experiment=xgb_exp)\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    study.optimize(lambda x: optimize(x, datasets), n_trials=10,\n",
        "                   callbacks=[xgb_neptune_callback])\n",
        "    joblib.dump(\n",
        "        study, f'HPO/xgb_hpo_{str(datetime.datetime.now().date())}.pkl')\n",
        "    print('Creating LightGBM Trials')\n",
        "    lgb_exp = neptune.create_experiment('LGBM_HPO')\n",
        "    lgbm_neptune_callback = opt_utils.NeptuneCallback(experiment=lgb_exp)\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    study.optimize(loptimize, n_trials=10, callbacks=[lgbm_neptune_callback])\n",
        "    joblib.dump(\n",
        "        study, f'HPO/lgb_hpo_{str(datetime.datetime.now().date())}.pkl')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "import os\n",
        "import copy\n",
        "import joblib\n",
        "import neptune\n",
        "import neptunecontrib.monitoring.optuna as opt_utils\n",
        "import optuna\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn as nn\n",
        "from optuna.integration import PyTorchLightningPruningCallback\n",
        "from pytorch_lightning import Callback\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "from torch.utils.data import Subset, BatchSampler, SequentialSampler, DataLoader\n",
        "import torch\n",
        "import numpy as np\n",
        "from models.resnet import ResNet as Classifier\n",
        "from data_loading.purged_group_time_series import PurgedGroupTimeSeriesSplit\n",
        "from data_loading.utils import load_data, preprocess_data, FinData, weighted_mean, seed_everything, calc_data_mean, \\\n",
        "    create_dataloaders, load_model\n",
        "from models.SupervisedAutoEncoder import SupAE, train_ae_model, create_hidden_rep\n",
        "\n",
        "\n",
        "class MetricsCallback(Callback):\n",
        "    \"\"\"PyTorch Lightning metric callback.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.metrics = []\n",
        "\n",
        "    def on_validation_end(self, trainer, pl_module):\n",
        "        self.metrics.append(trainer.callback_metrics)\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        nn.init.xavier_normal_(m.weight, nn.init.calculate_gain('leaky_relu'))\n",
        "        m.bias.data.fill_(1)\n",
        "\n",
        "\n",
        "def create_param_dict(trial, trial_file=None):\n",
        "    if trial and not trial_file:\n",
        "        dim_1 = trial.suggest_int('dim_1', 500, 2000)\n",
        "        dim_2 = trial.suggest_int('dim_2', 1000, 3000)\n",
        "        dim_3 = trial.suggest_int('dim_3', 1000, 3000)\n",
        "        dim_4 = trial.suggest_int('dim_4', 500, 1000)\n",
        "        dim_5 = trial.suggest_int('dim_5', 100, 250)\n",
        "        act_func = trial.suggest_categorical(\n",
        "            'activation', ['relu', 'leaky_relu', 'gelu', 'silu'])\n",
        "        act_dict = {'relu': nn.ReLU, 'leaky_relu': nn.LeakyReLU,\n",
        "                    'gelu': nn.GELU, 'silu': nn.SiLU}\n",
        "        act_func = act_dict[act_func]\n",
        "        dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n",
        "        lr = trial.suggest_uniform('lr', 0.00005, 0.05)\n",
        "        p = {'dim_1': dim_1, 'dim_2': dim_2, 'dim_3': dim_3,\n",
        "             'dim_4': dim_4, 'dim_5': dim_5, 'activation': act_func, 'dropout': dropout,\n",
        "             'lr': lr, 'loss': nn.MSELoss, 'embedding': True}\n",
        "    elif trial and trial_file:\n",
        "        p = joblib.load(trial_file).best_params\n",
        "        if not p.get('dim_5', None):\n",
        "            p['dim_5'] = 75\n",
        "        if not p.get('label_smoothing', None):\n",
        "            p['label_smoothing'] = 0.094\n",
        "        act_dict = {'relu': nn.ReLU,\n",
        "                    'leaky_relu': nn.LeakyReLU, 'gelu': nn.GELU}\n",
        "        act_func = trial.suggest_categorical(\n",
        "            'activation', ['leaky_relu', 'gelu'])\n",
        "        p['activation'] = act_dict[p['activation']]\n",
        "    return p\n",
        "\n",
        "\n",
        "def optimize(trial: optuna.Trial, data_dict):\n",
        "    gts = PurgedGroupTimeSeriesSplit(n_splits=5, group_gap=5)\n",
        "    input_size = data_dict['data'].shape[-1]\n",
        "    output_size = 1\n",
        "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "        os.path.join('hpo/trials/', \"trial_resnet_{}\".format(trial.number)), monitor=\"val_mse\", mode='min')\n",
        "    logger = MetricsCallback()\n",
        "    metrics = []\n",
        "    sizes = []\n",
        "    # trial_file = 'HPO/nn_hpo_2021-01-05.pkl'\n",
        "    trial_file = None\n",
        "    p = create_param_dict(trial, trial_file)\n",
        "    p['batch_size'] = trial.suggest_int('batch_size', 8000, 15000)\n",
        "    if data_dict.get('hidden_true', None):\n",
        "        p['hidden_len'] = data_dict['hidden']['hidden'].shape[-1]\n",
        "    for i, (train_idx, val_idx) in enumerate(gts.split(data_dict['data'], groups=data_dict['era'])):\n",
        "        model = Classifier(input_size, output_size, params=p)\n",
        "        # model.apply(init_weights)\n",
        "        dataset = FinData(\n",
        "            data=data_dict['data'], target=data_dict['target'], era=data_dict['era'], hidden=data_dict.get('hidden', None))\n",
        "        dataloaders = create_dataloaders(\n",
        "            dataset, indexes={'train': train_idx, 'val': val_idx}, batch_size=p['batch_size'])\n",
        "        es = EarlyStopping(monitor='val_mse', patience=10,\n",
        "                           min_delta=0.0005, mode='min')\n",
        "        print(\"pl.Trainer...\")\n",
        "        trainer = pl.Trainer(logger=False,\n",
        "                             max_epochs=500,\n",
        "                             gpus = 0,\n",
        "                             callbacks=[checkpoint_callback, logger, PyTorchLightningPruningCallback(\n",
        "                                 trial, monitor='val_mse'), es],\n",
        "                             precision=32)\n",
        "        print(\"Trainer.fit...\")\n",
        "        trainer.fit(\n",
        "            model, train_dataloader=dataloaders['train'], val_dataloaders=dataloaders['val'])\n",
        "        val_loss = logger.metrics[-1]['val_loss'].item()\n",
        "        metrics.append(val_loss)\n",
        "        sizes.append(len(train_idx))\n",
        "    metrics_mean = weighted_mean(metrics, sizes)\n",
        "    return metrics_mean\n",
        "\n",
        "\n",
        "def main(train_ae):\n",
        "    seed_everything(0)\n",
        "    data = load_data(root_dir='./data/', mode='train')\n",
        "    data, target, features, era = preprocess_data(\n",
        "        data, ordinal=True)\n",
        "    data_dict = {'data': data, 'target': target,\n",
        "                 'features': features, 'era': era}\n",
        "    if train_ae:\n",
        "        print(\"Train AE...\")\n",
        "        model = train_ae_model(data_dict=data_dict)\n",
        "    else:\n",
        "        print(\"Load AE Model...\")\n",
        "        p = joblib.load('./saved_models/parameters/ae_params.pkl')\n",
        "        p['input_size'] = len(data_dict['features'])\n",
        "        p['output_size'] = 1\n",
        "        model = load_model('./saved_models/trained/trained_ae.pth',\n",
        "                           p=p, pl_lightning=False, model=SupAE)\n",
        "\n",
        "    data_dict['hidden'] = create_hidden_rep(model, data_dict)\n",
        "    data_dict['hidden_true'] = True\n",
        "    api_token = 'eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzMGQ1MWZiNy1iYjNlLTQ3NDctOTE4OS1lNzhlNmVlYmUwMzYifQ=='\n",
        "    neptune.init(api_token=api_token,\n",
        "                 project_qualified_name='kramerji/Numerai')\n",
        "    nn_exp = neptune.create_experiment('Resnet_HPO')\n",
        "    nn_neptune_callback = opt_utils.NeptuneCallback(experiment=nn_exp)\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    print(\"Optuna Study...\")\n",
        "    study.optimize(lambda trial: optimize(trial, data_dict=data_dict), n_trials=100,\n",
        "                   callbacks=[nn_neptune_callback])\n",
        "    joblib.dump(\n",
        "        study, f'./hpo/params/nn_hpo_train_{train_ae}_{str(datetime.datetime.now().date())}.pkl')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "import gc\n",
        "import copy\n",
        "import datatable as dt\n",
        "import joblib\n",
        "import lightgbm as lgb\n",
        "import neptune\n",
        "import neptunecontrib.monitoring.optuna as opt_utils\n",
        "import numpy as np\n",
        "import optuna\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from data_loading import utils\n",
        "from data_loading import purged_group_time_series as pgs\n",
        "\n",
        "\n",
        "def optimize(trial: optuna.trial.Trial, data_dict: dict):\n",
        "    p = {'learning_rate':    trial.suggest_uniform('learning_rate', 1e-4, 1e-1),\n",
        "         'max_depth':        trial.suggest_int('max_depth', 5, 30),\n",
        "         'max_leaves':       trial.suggest_int('max_leaves', 5, 50),\n",
        "         'subsample':        trial.suggest_uniform('subsample', 0.3, 1.0),\n",
        "         'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.3, 1.0),\n",
        "         'min_child_weight': trial.suggest_int('min_child_weight', 5, 100),\n",
        "         'lambda':           trial.suggest_uniform('lambda', 0.05, 0.2),\n",
        "         'alpha':            trial.suggest_uniform('alpha', 0.05, 0.2),\n",
        "         'objective':        'reg:squarederror',\n",
        "         'booster':          'gbtree',\n",
        "         'tree_method':      'gpu_hist',\n",
        "         'verbosity':        1,\n",
        "         'n_jobs':           10,\n",
        "         'eval_metric':      'rmse'}\n",
        "    print('Choosing parameters:', p)\n",
        "    scores = []\n",
        "    sizes = []\n",
        "    # gts = GroupTimeSeriesSplit()']\n",
        "\n",
        "    gts = pgs.PurgedGroupTimeSeriesSplit(n_splits=5, group_gap=10)\n",
        "    for i, (tr_idx, val_idx) in enumerate(gts.split(data_dict['data'], groups=data_dict['era'])):\n",
        "        x_tr, x_val = data_dict['data'][tr_idx], data_dict['data'][val_idx]\n",
        "        y_tr, y_val = data_dict['target'][tr_idx], data_dict['target'][val_idx]\n",
        "        d_tr = xgb.DMatrix(x_tr, label=y_tr)\n",
        "        d_val = xgb.DMatrix(x_val, label=y_val)\n",
        "        clf = xgb.train(p, d_tr, 500, [\n",
        "            (d_val, 'eval')], early_stopping_rounds=50, verbose_eval=True)\n",
        "        val_pred = clf.predict(d_val)\n",
        "        score = mean_squared_error(y_val, val_pred)\n",
        "        scores.append(score)\n",
        "        sizes.append(len(tr_idx) + len(val_idx))\n",
        "        del clf, val_pred, d_tr, d_val, x_tr, x_val, y_tr, y_val, score\n",
        "        rubbish = gc.collect()\n",
        "    print(scores)\n",
        "    avg_score = utils.weighted_mean(scores, sizes)\n",
        "    print('Avg Score:', avg_score)\n",
        "    return avg_score\n",
        "\n",
        "\n",
        "def loptimize(trial, data_dict: dict):\n",
        "    p = {'learning_rate':    trial.suggest_uniform('learning_rate', 1e-4, 1e-1),\n",
        "         'max_leaves':       trial.suggest_int('max_leaves', 5, 100),\n",
        "         'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.3, 0.99),\n",
        "         'bagging_freq':     trial.suggest_int('bagging_freq', 1, 10),\n",
        "         'feature_fraction': trial.suggest_uniform('feature_fraction', 0.3, 0.99),\n",
        "         'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 50, 1000),\n",
        "         'lambda_l1':        trial.suggest_uniform('lambda_l1', 0.005, 0.05),\n",
        "         'lambda_l2':        trial.suggest_uniform('lambda_l2', 0.005, 0.05),\n",
        "         'boosting':         trial.suggest_categorical('boosting', ['gbdt', 'goss', 'rf']),\n",
        "         'objective':        'regression',\n",
        "         'verbose':          1,\n",
        "         'n_jobs':           10,\n",
        "         'metric':           'mse'}\n",
        "    if p['boosting'] == 'goss':\n",
        "        p['bagging_freq'] = 0\n",
        "        p['bagging_fraction'] = 1.0\n",
        "    scores = []\n",
        "    sizes = []\n",
        "    # gts = GroupTimeSeriesSplit()\n",
        "    gts = pgs.PurgedGroupTimeSeriesSplit(n_splits=5, group_gap=10)\n",
        "    for i, (tr_idx, val_idx) in enumerate(gts.split(data_dict['data'], groups=data_dict['era'])):\n",
        "        sizes.append(len(tr_idx) + len(val_idx))\n",
        "        x_tr, x_val = data_dict['data'][tr_idx], data_dict['data'][val_idx]\n",
        "        y_tr, y_val = data_dict['target'][tr_idx], data_dict['target'][val_idx]\n",
        "        train = lgb.Dataset(x_tr, label=y_tr)\n",
        "        val = lgb.Dataset(x_val, label=y_val)\n",
        "        clf = lgb.train(p, train, 500, valid_sets=[\n",
        "            val], early_stopping_rounds=50, verbose_eval=True)\n",
        "        preds = clf.predict(x_val)\n",
        "        score = mean_squared_error(y_val, preds)\n",
        "        scores.append(score)\n",
        "        del clf, preds, train, val, x_tr, x_val, y_tr, y_val, score\n",
        "        rubbish = gc.collect()\n",
        "    print(scores)\n",
        "    avg_score = utils.weighted_mean(scores, sizes)\n",
        "    print('Avg Score:', avg_score)\n",
        "    return avg_score\n",
        "\n",
        "\n",
        "def main():\n",
        "    api_token = 'eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzMGQ1MWZiNy1iYjNlLTQ3NDctOTE4OS1lNzhlNmVlYmUwMzYifQ=='\n",
        "    neptune.init(api_token=api_token,\n",
        "                 project_qualified_name='kramerji/Numerai')\n",
        "    data = utils.load_data('data/', mode='train')\n",
        "    data, target, features, era = utils.preprocess_data(data, nn=True)\n",
        "    data_dict = {'data':     data, 'target': target,\n",
        "                 'features': features, 'era': era}\n",
        "    print('creating XGBoost Trials')\n",
        "    xgb_exp = neptune.create_experiment('XGBoost_HPO')\n",
        "    xgb_neptune_callback = opt_utils.NeptuneCallback(experiment=xgb_exp)\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    study.optimize(lambda trial: optimize(trial, data_dict),\n",
        "                   n_trials=100, callbacks=[xgb_neptune_callback])\n",
        "    joblib.dump(\n",
        "        study, f'hpo/params/xgb_hpo_{str(datetime.datetime.now().date())}.pkl')\n",
        "    print('Creating LightGBM Trials')\n",
        "    lgb_exp = neptune.create_experiment('LGBM_HPO')\n",
        "    lgbm_neptune_callback = opt_utils.NeptuneCallback(experiment=lgb_exp)\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    study.optimize(lambda trial: loptimize(trial, data_dict),\n",
        "                   n_trials=100, callbacks=[lgbm_neptune_callback])\n",
        "    joblib.dump(\n",
        "        study, f'hpo/params/lgb_hpo_{str(datetime.datetime.now().date())}.pkl')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "import os\n",
        "import copy\n",
        "import joblib\n",
        "import neptune\n",
        "import neptunecontrib.monitoring.optuna as opt_utils\n",
        "import optuna\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn as nn\n",
        "from optuna.integration import PyTorchLightningPruningCallback\n",
        "from pytorch_lightning import Callback\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "from torch.utils.data import Subset, BatchSampler, SequentialSampler, DataLoader\n",
        "import torch\n",
        "import numpy as np\n",
        "from models.SupervisedAutoEncoder import SupAE\n",
        "from data_loading.purged_group_time_series import PurgedGroupTimeSeriesSplit\n",
        "from data_loading.utils import load_data, preprocess_data, FinData, weighted_mean, seed_everything, calc_data_mean, \\\n",
        "    create_dataloaders\n",
        "\n",
        "\n",
        "class MetricsCallback(Callback):\n",
        "    \"\"\"PyTorch Lightning metric callback.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.metrics = []\n",
        "\n",
        "    def on_validation_end(self, trainer, pl_module):\n",
        "        self.metrics.append(trainer.callback_metrics)\n",
        "\n",
        "\n",
        "def create_param_dict(trial, trial_file=None):\n",
        "    if trial and not trial_file:\n",
        "        dim_1 = trial.suggest_int('dim_1', 500, 1000)\n",
        "        dim_2 = trial.suggest_int('dim_2', 250, 500)\n",
        "        dim_3 = trial.suggest_int('dim_3', 100, 250)\n",
        "        hidden = trial.suggest_int('hidden', 50, 200)\n",
        "        act_func = trial.suggest_categorical(\n",
        "            'activation', ['relu', 'leaky_relu', 'gelu', 'silu'])\n",
        "        act_dict = {'relu': nn.ReLU, 'leaky_relu': nn.LeakyReLU,\n",
        "                    'gelu': nn.GELU, 'silu': nn.SiLU}\n",
        "        act_func = act_dict[act_func]\n",
        "        dropout = trial.suggest_uniform('dropout', 0.1, 0.5)\n",
        "        lr = trial.suggest_uniform('lr', 0.00005, 0.05)\n",
        "        recon_loss_factor = trial.suggest_uniform('recon_loss_factor', 0.1, 1)\n",
        "        p = {'dim_1': dim_1, 'dim_2': dim_2, 'dim_3': dim_3, 'hidden': hidden,\n",
        "             'activation': act_func, 'dropout': dropout,\n",
        "             'lr': lr, 'recon_loss_factor': recon_loss_factor, 'loss_sup_ae': nn.MSELoss,\n",
        "             'loss_recon': nn.MSELoss,\n",
        "             'embedding': True}\n",
        "    elif trial and trial_file:\n",
        "        p = joblib.load(trial_file).best_params\n",
        "        if not p.get('dim_5', None):\n",
        "            p['dim_5'] = 75\n",
        "        if not p.get('label_smoothing', None):\n",
        "            p['label_smoothing'] = 0.094\n",
        "        act_dict = {'relu': nn.ReLU,\n",
        "                    'leaky_relu': nn.LeakyReLU, 'gelu': nn.GELU}\n",
        "        act_func = trial.suggest_categorical(\n",
        "            'activation', ['leaky_relu', 'gelu'])\n",
        "        p['activation'] = act_dict[p['activation']]\n",
        "    return p\n",
        "\n",
        "\n",
        "def optimize(trial: optuna.Trial, data_dict):\n",
        "    gts = PurgedGroupTimeSeriesSplit(n_splits=5, group_gap=5)\n",
        "    input_size = data_dict['data'].shape[-1]\n",
        "    checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "        os.path.join('hpo/checkpoints/', \"trial_ae_{}\".format(trial.number)), monitor=\"val_sup_loss\", mode='min')\n",
        "    logger = MetricsCallback()\n",
        "    metrics = []\n",
        "    sizes = []\n",
        "    # trial_file = 'HPO/nn_hpo_2021-01-05.pkl'\n",
        "    trial_file = None\n",
        "    p = create_param_dict(trial, trial_file)\n",
        "    p['batch_size'] = trial.suggest_int('batch_size', 500, 2000)\n",
        "    p['input_size'] = input_size\n",
        "    p['output_size'] = 1\n",
        "    print(f'Running Trail with params: {p}')\n",
        "    for i, (train_idx, val_idx) in enumerate(gts.split(data_dict['data'], groups=data_dict['era'])):\n",
        "        model = SupAE(params=p)\n",
        "        # model.apply(init_weights)\n",
        "        dataset = FinData(\n",
        "            data=data_dict['data'], target=data_dict['target'], era=data_dict['era'])\n",
        "        dataloaders = create_dataloaders(\n",
        "            dataset, indexes={'train': train_idx, 'val': val_idx}, batch_size=p['batch_size'])\n",
        "        es = EarlyStopping(monitor='val_loss', patience=10,\n",
        "                           min_delta=0.0005, mode='min')\n",
        "        trainer = pl.Trainer(logger=False,\n",
        "                             max_epochs=100,\n",
        "                             gpus=1,\n",
        "                             callbacks=[checkpoint_callback, logger, PyTorchLightningPruningCallback(\n",
        "                                 trial, monitor='val_sup_loss'), es],\n",
        "                             precision=16)\n",
        "        trainer.fit(\n",
        "            model, train_dataloader=dataloaders['train'], val_dataloaders=dataloaders['val'])\n",
        "        val_loss = logger.metrics[-1]['val_sup_loss'].item()\n",
        "        metrics.append(val_loss)\n",
        "        sizes.append(len(train_idx))\n",
        "    metrics_mean = weighted_mean(metrics, sizes)\n",
        "    return metrics_mean\n",
        "\n",
        "\n",
        "def main():\n",
        "    seed_everything(0)\n",
        "    data = load_data(root_dir='./data/', mode='train')\n",
        "    data, target, features, era = preprocess_data(\n",
        "        data, ordinal=True)\n",
        "    api_token = 'eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzMGQ1MWZiNy1iYjNlLTQ3NDctOTE4OS1lNzhlNmVlYmUwMzYifQ=='\n",
        "    neptune.init(api_token=api_token,\n",
        "                 project_qualified_name='kramerji/Numerai')\n",
        "    nn_exp = neptune.create_experiment('SupAE_HPO')\n",
        "    nn_neptune_callback = opt_utils.NeptuneCallback(experiment=nn_exp)\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    data_dict = {'data': data, 'target': target,\n",
        "                 'features': features, 'era': era}\n",
        "    study.optimize(lambda trial: optimize(trial, data_dict=data_dict), n_trials=100,\n",
        "                   callbacks=[nn_neptune_callback])\n",
        "    joblib.dump(\n",
        "        study, f'hpo/params/SupAEnn_hpo_{str(datetime.datetime.now().date())}.pkl')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from hpo import gbm_hpo, nn_hpo, ae_hpo\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import numerapi\n",
        "\n",
        "\n",
        "def credentials():\n",
        "    dotenv_path = 'num_config.env'\n",
        "    load_dotenv(dotenv_path=dotenv_path)\n",
        "    pub_id = os.getenv('PUBLIC_ID')\n",
        "    priv_key = os.getenv('PRIVATE_KEY')\n",
        "    latest_round = os.getenv('LATEST_ROUND')\n",
        "    #pub_id = \"C4Q5XUHAH3MSMAHHHRHSQWHG2SUW5Q54\"\n",
        "    #priv_key = \"QSBJY72HUPPDJBP3UF7JPFHRK4CV3ITWOEWQNWXD44RUTVORXQCB5BYIQK7I4CMD\"\n",
        "    #latest_round = 156\n",
        "    return {'PUBLIC_ID': pub_id, 'PRIVATE_KEY': priv_key, 'LATEST_ROUND': latest_round}\n",
        "\n",
        "\n",
        "def download_data(api: numerapi.NumerAPI, keys):\n",
        "    if int(keys['LATEST_ROUND']) == api.get_current_round():\n",
        "        return int(keys['LATEST_ROUND'])\n",
        "    else:\n",
        "        LATEST_ROUND = api.get_current_round()\n",
        "        api.download_current_dataset('./data')\n",
        "        return LATEST_ROUND\n",
        "\n",
        "\n",
        "def update_env_file(env_vars):\n",
        "    with open('num_config.env', 'w') as f:\n",
        "        f.write(f'LATEST_ROUND={env_vars[\"LATEST_ROUND\"]}\\n')\n",
        "        f.write(f'PUBLIC_ID={env_vars[\"PUBLIC_ID\"]}\\n')\n",
        "        f.write(f'PRIVATE_KEY={env_vars[\"PRIVATE_KEY\"]}\\n')\n",
        "\n",
        "def create_preds():\n",
        "    pass\n",
        "def main():\n",
        "    keys = credentials()\n",
        "    numapi = numerapi.NumerAPI(\n",
        "        verbosity='INFO', public_id=keys['PUBLIC_ID'], secret_key=keys['PRIVATE_KEY'])\n",
        "    keys['LATEST_ROUND'] = download_data(numapi, keys)\n",
        "    update_env_file(keys)\n",
        "    #gbm_hpo.main()\n",
        "    #ae_hpo.main()\n",
        "    print(\"nn_hpo...\")\n",
        "    nn_hpo.main(train_ae=False)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}