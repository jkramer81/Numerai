{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from logging import root\n",
        "from models.SupervisedAutoEncoder import create_hidden_rep\n",
        "from operator import mod\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "from typing import List, Tuple\n",
        "import dotenv\n",
        "import datatable as dt\n",
        "from dotenv.main import load_dotenv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
        "from torch.utils.data import Dataset, Subset, BatchSampler, SequentialSampler, DataLoader\n",
        "\n",
        "\n",
        "# from lightning_nn import Classifier\n",
        "\n",
        "\n",
        "class FinData(Dataset):\n",
        "    def __init__(self, data, target, era, hidden=None, mode='train', transform=None, cache_dir=None):\n",
        "        self.data = data\n",
        "        self.target = target\n",
        "        self.mode = mode\n",
        "        self.transform = transform\n",
        "        self.cache_dir = cache_dir\n",
        "        self.era = era\n",
        "        self.hidden = hidden\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index.to_list()\n",
        "        if self.transform:\n",
        "            return self.transform(self.data.iloc[index].values)\n",
        "        else:\n",
        "            if type(index) is list:\n",
        "                sample = {\n",
        "                    'target': torch.Tensor(self.target[index].values),\n",
        "                    'data':   torch.LongTensor(self.data[index]),\n",
        "                    'era':    torch.Tensor(self.era[index].values),\n",
        "                }\n",
        "            else:\n",
        "                sample = {\n",
        "                    'target': torch.Tensor([self.target[index]]),\n",
        "                    'data':   torch.LongTensor([self.data[index]]),\n",
        "                    'era':    torch.Tensor([self.era[index]]),\n",
        "                }\n",
        "            if self.hidden is not None:\n",
        "                sample['hidden'] = torch.Tensor(self.hidden['hidden'][index])\n",
        "        return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def get_data_path(root_dir):\n",
        "    dotenv_path = 'num_config.env'\n",
        "    load_dotenv(dotenv_path=dotenv_path)\n",
        "    curr_round = os.getenv('LATEST_ROUND')\n",
        "    data_path = root_dir + '/numerai_dataset_' + str(curr_round)\n",
        "    return data_path\n",
        "\n",
        "\n",
        "def load_data(root_dir, mode, overide=None):\n",
        "    data_path = get_data_path(root_dir=root_dir)\n",
        "    if overide:\n",
        "        data = dt.fread(overide).to_pandas()\n",
        "    elif mode == 'train':\n",
        "        data = dt.fread(data_path + '/numerai_training_data.csv').to_pandas()\n",
        "    elif mode == 'test':\n",
        "        data = dt.fread(data_path + '/numerai_tournament_data.csv').to_pandas()\n",
        "    return data\n",
        "\n",
        "\n",
        "def preprocess_data(data: pd.DataFrame, scale: bool = False, nn: bool = False, test=False, ordinal=False):\n",
        "    \"\"\"\n",
        "    Preprocess the data.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data\n",
        "        Pandas DataFrame\n",
        "    scale\n",
        "        scale data with unit std and 0 mean\n",
        "    nn\n",
        "        return data as np.array\n",
        "    missing\n",
        "        options to replace missing data with - mean, median, 0\n",
        "    action\n",
        "        options to create action value  - weight = (weight * resp) > 0\n",
        "                                        - combined = (resp_cols) > 0\n",
        "                                        - multi = each resp cols >0\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    \"\"\"\n",
        "    features = [col for col in data.columns if 'feature' in col]\n",
        "    era = data['era']\n",
        "    era = era.transform(lambda x: re.sub('[a-z]', '', x))\n",
        "    if not test:\n",
        "        era = era.astype('int')\n",
        "    target = data['target']\n",
        "    data = data[features]\n",
        "    if scale:\n",
        "        scaler = StandardScaler()\n",
        "        data = scaler.fit_transform(data)\n",
        "    if ordinal:\n",
        "        oe = OrdinalEncoder()\n",
        "        data = oe.fit_transform(data)\n",
        "        # data = data.values\n",
        "    if nn:\n",
        "        data = data.values\n",
        "    return data, target, features, era\n",
        "\n",
        "\n",
        "def calc_data_mean(array, cache_dir=None, fold=None, train=True, mode='mean'):\n",
        "    if train:\n",
        "        if mode == 'mean':\n",
        "            f_mean = np.nanmean(array, axis=0)\n",
        "            if cache_dir and fold:\n",
        "                np.save(f'{cache_dir}/f_{fold}_mean.npy', f_mean)\n",
        "            elif cache_dir:\n",
        "                np.save(f'{cache_dir}/f_mean.npy', f_mean)\n",
        "            array = np.nan_to_num(array) + np.isnan(array) * f_mean\n",
        "        if mode == 'median':\n",
        "            f_med = np.nanmedian(array, axis=0)\n",
        "            if cache_dir and fold:\n",
        "                np.save(f'{cache_dir}/f_{fold}_median.npy', f_med)\n",
        "            elif cache_dir:\n",
        "                np.save(f'{cache_dir}/f_median.npy', f_med)\n",
        "            array = np.nan_to_num(array) + np.isnan(array) * f_med\n",
        "        if mode == 'zero':\n",
        "            array = np.nan_to_num(array) + np.isnan(array) * 0\n",
        "    if not train:\n",
        "        if mode == 'mean':\n",
        "            f_mean = np.load(f'{cache_dir}/f_mean.npy')\n",
        "            array = np.nan_to_num(array) + np.isnan(array) * f_mean\n",
        "        if mode == 'median':\n",
        "            f_med = np.load(f'{cache_dir}/f_med.npy')\n",
        "            array = np.nan_to_num(array) + np.isnan(array) * f_med\n",
        "        if mode == 'zero':\n",
        "            array = np.nan_to_num(array) + np.isnan(array) * 0\n",
        "    return array\n",
        "\n",
        "\n",
        "def weighted_mean(scores, sizes):\n",
        "    largest = np.max(sizes)\n",
        "    weights = [size / largest for size in sizes]\n",
        "    return np.average(scores, weights=weights)\n",
        "\n",
        "\n",
        "def create_dataloaders(dataset: Dataset, indexes: dict, batch_size):\n",
        "    train_idx = indexes.get('train', None)\n",
        "    val_idx = indexes.get('val', None)\n",
        "    test_idx = indexes.get('test', None)\n",
        "    dataloaders = {}\n",
        "    if train_idx:\n",
        "        train_set = Subset(dataset, train_idx)\n",
        "        train_sampler = BatchSampler(\n",
        "            train_set.indices, batch_size=batch_size, drop_last=False)\n",
        "        dataloaders['train'] = DataLoader(\n",
        "            dataset, sampler=train_sampler, num_workers=2, pin_memory=False, shuffle=False)\n",
        "    if val_idx:\n",
        "        val_set = Subset(dataset, val_idx)\n",
        "        val_sampler = BatchSampler(\n",
        "            val_set.indices, batch_size=batch_size, drop_last=False)\n",
        "        dataloaders['val'] = DataLoader(\n",
        "            dataset, sampler=val_sampler, num_workers=2, pin_memory=False, shuffle=False)\n",
        "    if test_idx:\n",
        "        test_set = Subset(dataset, test_idx)\n",
        "        test_sampler = BatchSampler(\n",
        "            test_set.indices, batch_size=batch_size, drop_last=False)\n",
        "        dataloaders['test'] = DataLoader(\n",
        "            dataset, sampler=test_sampler, num_workers=2, pin_memory=False, shuffle=False)\n",
        "    return dataloaders\n",
        "\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "\n",
        "def load_model(path, p, pl_lightning, model):\n",
        "    Classifier = model\n",
        "    if os.path.isdir(path):\n",
        "        models = []\n",
        "        for file in os.listdir(path):\n",
        "            if pl_lightning:\n",
        "                model = Classifier.load_from_checkpoint(\n",
        "                    checkpoint_path=path + '/' + file, params=p)\n",
        "            else:\n",
        "                model = Classifier(params=p)\n",
        "                model.load_state_dict(torch.load(path + '/' + file))\n",
        "            models.append(model)\n",
        "        return models\n",
        "    elif os.path.isfile(path):\n",
        "        if pl_lightning:\n",
        "            return Classifier.load_from_checkpoint(checkpoint_path=path, params=p)\n",
        "        else:\n",
        "            model = Classifier(params=p)\n",
        "            model.load_state_dict(torch.load(path))\n",
        "            return model\n",
        "\n",
        "\n",
        "def init_weights(m, func):\n",
        "    if type(m) == nn.Linear:\n",
        "        nn.init.xavier_normal_(m.weight, nn.init.calculate_gain(func))\n",
        "\n",
        "\n",
        "def create_predictions(root_dir: str = './data', models: dict = {}, hidden=True, ae=True):\n",
        "    test_files_path, test_files_exist = check_test_files(root_dir)\n",
        "    if test_files_exist:\n",
        "        test_files = os.listdir(test_files_path)\n",
        "    for file in test_files:\n",
        "        df = load_data(root_dir='./data', mode='test',\n",
        "                       overide=f'{test_files_path}/{file}')\n",
        "        df['target'] = 0\n",
        "        data, target, features, era = preprocess_data(data=df, ordinal=True)\n",
        "        t_idx = np.arange(start=0, stop=len(era), step=1).tolist()\n",
        "        data_dict = data_dict = {'data': data, 'target': target,\n",
        "                                 'features': features, 'era': era}\n",
        "        if models.get('ae', None):\n",
        "            p_ae = models['ae'][1]\n",
        "            p_ae['input_size'] = len(features)\n",
        "            p_ae['output_size'] = 1\n",
        "            model = models['ae'][0]\n",
        "            model.eval()\n",
        "        if not ae:\n",
        "            hidden_pred = create_hidden_rep(\n",
        "                model=model, data_dict=data_dict)\n",
        "            data_dict['hidden_true'] = True\n",
        "            df['prediction_ae'] = hidden_pred['preds']\n",
        "        if models.get('ResNet', None):\n",
        "            p_res = models['ResNet'][1]\n",
        "            p_res['input_size'] = len(features)\n",
        "            p_res['output_size'] = 1\n",
        "            p_res['hidden_len'] = data_dict['hidden'].shape[-1]\n",
        "            dataset = FinData(\n",
        "                data=data_dict['data'], target=data_dict['target'], era=data_dict['era'], hidden=data_dict.get('hidden', None))\n",
        "            dataloaders = create_dataloaders(\n",
        "                dataset, indexes={'train': t_idx}, batch_size=p_res['batch_size'])\n",
        "            model = models['ResNet'][0]\n",
        "            model.eval()\n",
        "            predictions = []\n",
        "            for batch in dataloaders['train']:\n",
        "                pred = model(batch['data'].view(\n",
        "                    batch['data'].shape[1], -1), hidden=batch['hidden'].view(batch['hidden'].shape[1], -1))\n",
        "                predictions.append(pred.cpu().detach().numpy().tolist())\n",
        "            predictions = np.array([predictions[i][j] for i in range(\n",
        "                len(predictions)) for j in range(len(predictions[i]))])\n",
        "            df['prediction_resnet'] = predictions\n",
        "        if models.get('xgboost', None):\n",
        "            model_xgboost = models['xgboost'][0]\n",
        "            p_xgboost = models['xgboost'][1]\n",
        "            x_val = data_dict['data']\n",
        "            df['prediction_xgb'] = model_xgboost.predict(x_val)\n",
        "        if models.get('lgb', None):\n",
        "            model_lgb = models['lgb'][0]\n",
        "            p_lgb = models['lgb'][1]\n",
        "            x_val = data_dict['data']\n",
        "            df['prediction_lgb'] = model_lgb.predict(x_val)\n",
        "            df = df[['id', 'prediction_lgb']]\n",
        "        pred_path = f'{get_data_path(root_dir)}/predictions/{era[0]}'\n",
        "        df.to_csv(f'{pred_path}.csv')\n",
        "\n",
        "\n",
        "def check_test_files(root_dir='./data'):\n",
        "    data_path = get_data_path(root_dir)\n",
        "    test_files_path = f'{data_path}/test_files'\n",
        "    # TODO Check to make sure all era's present\n",
        "    if os.path.isdir(test_files_path):\n",
        "        return test_files_path, True\n",
        "    else:\n",
        "        os.makedirs(test_files_path)\n",
        "        df = load_data(root_dir=root_dir, mode='test')\n",
        "        df['era'][df['era'] == 'eraX'] = 'era999'\n",
        "        for era in df['era'].unique():\n",
        "            path = f'{test_files_path}/{era}'\n",
        "            df[df['era'] == era].to_csv(f'{path}.csv')\n",
        "        return test_files_path, True\n",
        "\n",
        "\n",
        "def create_prediction_file(root_dir='./data', eras=None):\n",
        "    pred_path = f'{get_data_path(root_dir)}/predictions/'\n",
        "    files = os.listdir(pred_path)\n",
        "    files.sort()\n",
        "    if eras:\n",
        "        dfs = [pd.read_csv(f'{pred_path}{file}')\n",
        "               for file in files if file != 'predictions.csv' and file in eras]\n",
        "    else:\n",
        "        dfs = [pd.read_csv(f'{pred_path}{file}')\n",
        "               for file in files if file != 'predictions.csv']\n",
        "    df = pd.concat(dfs)\n",
        "    df = df[['id', 'prediction_lgb']]\n",
        "    df.columns = ['id', 'prediction']\n",
        "    df.to_csv(f'{pred_path}predictions.csv')\n",
        "\n",
        "    return df"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}