{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd0ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963",
   "display_name": "Python 3.9.4 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .time    { background: #40CC40; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tbody td { text-align: left; }\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .sp {  opacity: 0.25;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "import copy\n",
    "from data_loading import utils, purged_group_time_series\n",
    "from data_loading.utils import load_data, preprocess_data, FinData, weighted_mean, seed_everything, calc_data_mean, \\\n",
    "    create_dataloaders\n",
    "from models import resnet, lightning_nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import joblib\n",
    "from models.SupervisedAutoEncoder import SupAE, create_hidden_rep\n",
    "import os\n",
    "from data_loading import purged_group_time_series as pgs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'p' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-67c245c30a95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_sets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'p' is not defined"
     ]
    }
   ],
   "source": [
    "#p_ae = {'dim_1': 675, 'dim_2': 400, 'dim_3': 224, 'hidden': 162,\n",
    "#         'activation': nn.ReLU, 'dropout': 0.2916447561918717, 'lr': 0.030272591341587315,\n",
    "#         'recon_loss_factor': 0.4447516076774931, 'batch_size': 1252, 'loss_sup_ae': nn.MSELoss,\n",
    "#         'loss_recon': nn.MSELoss,\n",
    "#         'embedding': True,\n",
    "#         'input_size':310,'output_size':1}\n",
    "\n",
    "#p_res = {'dim_1': 843, 'dim_2': 2637, 'dim_3': 1618, 'dim_4': 880, 'dim_5': 220, 'activation': nn.LeakyReLU, 'dropout': 0.453246718032545, 'lr': 0.04565788979670108, 'batch_size': 10836,'loss':nn.MSELoss,'embedding':True,'input_size':310,'output_size':1,'hidden_len':p_ae['hidden']}\n",
    "#model_res = resnet.ResNet(input_size=p_res['input_size'],output_size=p_res['output_size'],params=p_res)\n",
    "#model_res.load_state_dict(torch.load('./saved_models/trained/ResNet_state_dict.pth'))\n",
    "#model_ae = utils.load_model(path='./saved_models/trained/trained_ae.pth',p=p_ae,pl_lightning=False,model=SupAE)\n",
    "p_lgbm = \t{'learning_rate': 0.010758658263835987, 'max_leaves': 80, 'bagging_fraction': 0.4927572426577223, 'bagging_freq': 10, 'feature_fraction': 0.4023243616563433, 'min_data_in_leaf': 686, 'lambda_l1': 0.04534825564399065, 'lambda_l2': 0.0409642610305239, 'boosting': 'gbdt'}\n",
    "data = utils.load_data('data/', mode='train')\n",
    "data, target, features, era = utils.preprocess_data(data, nn=True)\n",
    "data_dict = {'data':     data, 'target': target,\n",
    "                 'features': features, 'era': era}\n",
    "scores = []\n",
    "sizes = []\n",
    "# gts = GroupTimeSeriesSplit()\n",
    "gts = pgs.PurgedGroupTimeSeriesSplit(n_splits=5, group_gap=10)\n",
    "seed_everything(0)\n",
    "for i, (tr_idx, val_idx) in enumerate(gts.split(data_dict['data'], groups=data_dict['era'])):\n",
    "    sizes.append(len(tr_idx) + len(val_idx))\n",
    "    x_tr, x_val = data_dict['data'][tr_idx], data_dict['data'][val_idx]\n",
    "    y_tr, y_val = data_dict['target'][tr_idx], data_dict['target'][val_idx]\n",
    "    train = lgb.Dataset(x_tr, label=y_tr)\n",
    "    val = lgb.Dataset(x_val, label=y_val)\n",
    "    clf = lgb.train(p_lgbm, train, 500, valid_sets=[val], early_stopping_rounds=50, verbose_eval=True)\n",
    "    preds = clf.predict(x_val)\n",
    "    score = mean_squared_error(y_val, preds)\n",
    "    scores.append(score)\n",
    "    #del clf, preds, train, val, x_tr, x_val, y_tr, y_val, score\n",
    "    rubbish = gc.collect()\n",
    "model_lgbm = clf\n",
    "models_dict = {'lgb':[model_lgbm, p_lgbm]}\n",
    "#models_dict = {'ResNet':[model_res,p_res],'ae':[model_ae,p_ae]}\n",
    "#models_dict = {'ae':[model_ae,p_ae]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "utils.create_predictions(models=models_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.create_prediction_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_path = f\"{utils.get_data_path(root_dir='./data')}/predictions/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.load_data(root_dir='./data/',mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_eras = df[df['data_type']=='validation']['era'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eras = os.listdir(pred_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eras = [f\"era{era.replace('.csv','')}\" for era in eras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eras = [era for era in eras if era in val_eras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eras =[era.replace('era','') for era in eras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eras = [f'{era}.csv' for era in eras]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = utils.create_prediction_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(predictions, targets):\n",
    "    ranked_preds = predictions.rank(pct=True, method=\"first\")\n",
    "    return np.corrcoef(ranked_preds, targets)[0, 1]\n",
    "\n",
    "\n",
    "# convenience method for scoring\n",
    "def scoring(df):\n",
    "    return correlation(df['prediction'], df['target'])\n",
    "\n",
    "\n",
    "# Payout is just the score cliped at +/-25%\n",
    "def payout(scores):\n",
    "    return scores.clip(lower=-0.25, upper=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(df_preds, on = 'id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = df.groupby('era').apply(scoring)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.load_data(root_dir='./data/',mode='test')\n",
    "df = df[df['data_type'] == 'validation']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.load_data(root_dir='./data/',mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['era'][df['era'] == 'eraX'] = 'era999'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,target,features,era = utils.preprocess_data(df,ordinal=True)\n",
    "data_test,target_test,features_test,era_test = utils.preprocess_data(df_test,ordinal=True)\n",
    "\"\"\"\n",
    "data_ = copy.deepcopy(data)\n",
    "target_ = copy.deepcopy(target)\n",
    "oe = OrdinalEncoder()\n",
    "data = oe.fit_transform(data)\n",
    "target=oe.fit_transform(target_.values.reshape(-1,1)).reshape(-1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df ,df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Autoencodr training\n",
    "data = np.concatenate([data,data_test],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.concatenate([target,target_test],0)\n",
    "era = np.concatenate([era,era_test],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t_idx = np.arange(start=0,stop=len(era),step=1).tolist()\n",
    "t_idx =np.where(era <121)[0].tolist()\n",
    "v_idx =np.where(era >=121)[0].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = joblib.load('./hpo/params/SupAEnn_hpo_2021-04-05.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = p.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p['activation'] = nn.ReLU\n",
    "p['loss_sup_ae']= nn.MSELoss\n",
    "p['loss_recon']= nn.MSELoss\n",
    "p['embedding']=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = utils.FinData(data = data,target=target,era=era)\n",
    "dataloaders = utils.create_dataloaders(dataset,indexes={'train':t_idx,'val':v_idx},batch_size=p['batch_size'])\n",
    "p['input_size'] = len(features)\n",
    "p['output_size'] = 1\n",
    "model = SupAE(params=p)\n",
    "es = EarlyStopping(monitor='val_sup_loss',patience=10,min_delta=0.0005,mode='min')\n",
    "trainer = pl.Trainer(max_epochs=100,gpus=1,callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch = next(iter(dataloaders['val']))\n",
    "batch['era']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.fit(model,train_dataloader = dataloaders['train'],val_dataloaders=dataloaders['val'])\n",
    "torch.save(model.state_dict(),f'./saved_models/trained/ae_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_xgb = joblib.load('./hpo/params/xgb_hpo_2021-04-05.pkl').best_params\n",
    "p_lgb = joblib.load('./hpo/params/lgb_hpo_2021-04-05.pkl').best_params\n",
    "p_lgb['boosting']= 'gbdt'\n",
    "p_lgb['max_depth']= 50\n",
    "p_lgb['objective']= 'regression'\n",
    "p_lgb['metric']= 'mse'\n",
    "p_lgb['n_jobs']=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = data_dict = {'data': data, 'target': target,\n",
    "                                 'features': features, 'era': era}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_val = data_dict['data'][t_idx], data_dict['data'][v_idx]\n",
    "y_tr, y_val = data_dict['target'][t_idx],data_dict['target'][v_idx]\n",
    "d_tr = lgb.Dataset(x_tr,label=y_tr)\n",
    "d_val = lgb.Dataset(x_val,label=y_val)\n",
    "clf = lgb.train(p_lgb,d_tr,1000,valid_sets=[d_val],early_stopping_rounds=50,verbose_eval=True)\n",
    "clf.save_model(f'./saved_models/trained/lgb.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {'data': data, 'target': target,\n",
    "                 'features': features, 'era': era}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ae['input_size'] = len(features)\n",
    "p_ae['output_size'] = 1\n",
    "model = utils.load_model('./saved_models/trained/trained_ae.pth',\n",
    "                    p=p_ae, pl_lightning=False, model=SupAE)\n"
   ]
  },
  {
   "source": [
    "\n",
    "models_dict = {'lgb':[clf,p_lgb]}"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_output = create_hidden_rep(model, data_dict)\n",
    "data_dict['hidden'],ae_preds = ae_output['hidden'], ae_output['preds']\n",
    "data_dict['hidden_true'] = True\n",
    "p['hidden_len'] = data_dict['hidden'].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = utils.FinData(\n",
    "            data=data_dict['data'], target=data_dict['target'], era=data_dict['era'], hidden=data_dict.get('hidden', None))\n",
    "dataloaders = utils.create_dataloaders(\n",
    "            dataset, indexes={'train': t_idx}, batch_size=p['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_res = {'dim_1': 843, 'dim_2': 2637, 'dim_3': 1618, 'dim_4': 880, 'dim_5': 220, 'activation': nn.LeakyReLU, 'dropout': 0.453246718032545, 'lr': 0.04565788979670108, 'batch_size': 10836,'loss':nn.MSELoss,'embedding':True,'input_size':310,'output_size':1,'hidden_len':p_ae['hidden']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet.ResNet(input_size = p['input_size'],output_size=p['output_size'],params=p)\n",
    "es = EarlyStopping(monitor='val_mse',patience=10,min_delta=0.0005,mode='min')\n",
    "trainer = pl.Trainer(max_epochs=100,gpus=1,callbacks=[es])\n",
    "trainer.fit(model,train_dataloader=dataloaders['train'],val_dataloaders=dataloaders['val'])\n",
    "torch.save(model.state_dict(),f'./saved_models/trained/ResNet_state_dict.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = utils.load_model('./saved_models/trained/ResNet_state_dict.pth',\n",
    " #                   p=p, pl_lightning=False, model=resnet.ResNet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_res = resnet.ResNet(input_size=p_res['input_size'],output_size=p_res['output_size'],params=p_res)\n",
    "model_res.load_state_dict(torch.load('./saved_models/trained/ResNet_state_dict.pth'))\n",
    "model_ae = utils.load_model(path='./saved_models/trained/trained_ae.pth',p=p_ae,pl_lightning=False,model=SupAE)\n",
    "models_dict = {'ResNet':[model_res,p_res],'ae':[model_ae,p_ae]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.create_predictions(models=models_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "p['input_size'] = len(features)\n",
    "p['output_size'] = 1\n",
    "train = True\n",
    "if train:\n",
    "    gts = purged_group_time_series.PurgedGroupTimeSeriesSplit(n_splits=5,group_gap=5)\n",
    "    models=[]\n",
    "    for i, (train_idx,val_idx) in enumerate(gts.split(data,groups=era)):\n",
    "        dataset = utils.FinData(data=data, target=target, era=era)\n",
    "        dataloaders = utils.create_dataloaders(\n",
    "        dataset, indexes={'train': train_idx, 'val': val_idx}, batch_size=p['batch_size'])\n",
    "        model = SupAE(p)\n",
    "        es = EarlyStopping(monitor='val_loss', patience=10,\n",
    "                            min_delta=0.005, mode='min')\n",
    "        trainer = pl.Trainer(max_epochs=100,\n",
    "                                gpus=1,\n",
    "                                callbacks=[es])\n",
    "        trainer.fit(\n",
    "            model, train_dataloader=dataloaders['train'], val_dataloaders=dataloaders['val'])\n",
    "        torch.save(model.state_dict(), f'saved_models/ae_fold_{i}_state_dict.pth')\n",
    "        models.append(model)\n",
    "else:\n",
    "    models_nn = utils.load_model('./saved_models/AE',p=p,pl_lightning=False,model=SupAE)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "outputPrepend"
    ]
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_,_,_,_ = utils.preprocess_data(df,nn=True)\n",
    "\n",
    "p = {'learning_rate': 0.030803514080008098,\n",
    "         'max_leaves': 50,\n",
    "         'bagging_fraction': 0.9011886437667906,\n",
    "         'bagging_freq': 5,\n",
    "         'feature_fraction': 0.3287921216266973,\n",
    "         'min_data_in_leaf': 396,\n",
    "         'lambda_l1': 0.02217696438630042,\n",
    "         'lambda_l2': 0.03985756503899372,\n",
    "         'boosting': 'gbdt',\n",
    "         'max_depth': 50,\n",
    "         'objective': 'regression',\n",
    "         'metric': 'mse',\n",
    "         'n_jobs':-1}\n",
    "gts = purged_group_time_series.PurgedGroupTimeSeriesSplit(n_splits=5,group_gap=5)\n",
    "models_gbm = []\n",
    "scores = []\n",
    "for i, (tr_idx,val_idx) in enumerate(gts.split(data_,groups=era)):\n",
    "    x_tr, x_val = data_[tr_idx], data_[val_idx]\n",
    "    y_tr, y_val = target[tr_idx],target[val_idx]\n",
    "    d_tr = lgb.Dataset(x_tr,label=y_tr)\n",
    "    d_val = lgb.Dataset(x_val,label=y_val)\n",
    "    clf = lgb.train(p,d_tr,1000,valid_sets=[d_val],early_stopping_rounds=50,verbose_eval=True)\n",
    "    preds = clf.predict(x_val)\n",
    "    score = mean_squared_error(y_val,preds)\n",
    "    print(f'Fold {i} MSE score:\\t', score)\n",
    "    models_gbm.append(clf)\n",
    "    scores.append(score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_,df,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = data\n",
    "output_size= 1\n",
    "p = {'dim_1': 1500,\n",
    "    'dim_2': 1000,\n",
    "    'dim_3': 500,\n",
    "    'dim_4': 250,\n",
    "    'dim_5': 150,\n",
    "    'activation': nn.SiLU,\n",
    "    'dropout': 0.2,\n",
    "    'lr': 0.05,\n",
    "    'label_smoothing':0.1,\n",
    "    'amsgrad': False,\n",
    "    'batch_size': 5000,\n",
    "    'loss':nn.MSELoss(),\n",
    "    'embedding':True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = utils.FinData(data=data, target=target, era=era)\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models_nn[-1]\n",
    "model.eval().to('cuda')\n",
    "index = np.linspace(0,dataset.data.shape[0],dataset.data.shape[0],dtype='int')"
   ]
  },
  {
   "source": [
    "batch_size = 5000\n",
    "dataloaders = utils.create_dataloaders(dataset,{'train':index.tolist()},batch_size=batch_size)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddens =[]\n",
    "for i, batch in enumerate(dataloaders['train']):\n",
    "    _,hidden,_,_ = model(batch['data'].view(batch['data'].size(1),-1).to('cuda'))\n",
    "    hiddens.append(hidden.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddens = np.array([hiddens[i][j] for i in range(len(hiddens)) for j in range(len(hiddens[i]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = np.concatenate([data,hiddens],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gts = purged_group_time_series.PurgedGroupTimeSeriesSplit(n_splits=5,group_gap=5)\n",
    "dataset = utils.FinData(data=data, target=target, era=era)\n",
    "models=[]\n",
    "for i, (train_idx,val_idx) in enumerate(gts.split(data,groups=era)):\n",
    "    \n",
    "    dataloaders = utils.create_dataloaders(\n",
    "    dataset, indexes={'train': train_idx, 'val': val_idx}, batch_size=p['batch_size'])\n",
    "    model = resnet.ResNet(input_size=input_size,output_size=output_size,params=p)\n",
    "    #model.apply(lambda x: utils.init_weights(x,'relu'))\n",
    "    es = EarlyStopping(monitor='val_mse', patience=10,\n",
    "                        min_delta=0.005, mode='min')\n",
    "    trainer = pl.Trainer(max_epochs=100,\n",
    "                            gpus=1,\n",
    "                            callbacks=[es])\n",
    "    trainer.fit(\n",
    "        model, train_dataloader=dataloaders['train'], val_dataloaders=dataloaders['val'])\n",
    "    torch.save(model.state_dict(), f'saved_models/Resnet/fold_{i}_state_dict.pth')\n",
    "    models.append(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = utils.load_data(root_dir='./data/',mode='test')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data,target,features,era = utils.preprocess_data(df.iloc[:10000,],test=True,ordinal=True)\n",
    "data_,_,_,_=utils.preprocess_data(df.iloc[:10000,],test=True,nn=True)\n",
    "\"\"\"\n",
    "data = data[0:1000]\n",
    "oe = OrdinalEncoder()\n",
    "data = oe.fit_transform(data)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = torch.tensor()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.LongTensor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(predictions, targets):\n",
    "    ranked_preds = predictions.rank(pct=True, method=\"first\")\n",
    "    return np.corrcoef(ranked_preds, targets)[0, 1]\n",
    "\n",
    "\n",
    "# convenience method for scoring\n",
    "def scoring(df):\n",
    "    return correlation(df['preds'], df['target'])\n",
    "\n",
    "\n",
    "# Payout is just the score cliped at +/-25%\n",
    "def payout(scores):\n",
    "    return scores.clip(lower=-0.25, upper=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for model in models_nn:\n",
    "    model.eval()\n",
    "    preds.append(model(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [preds[i][1].detach().numpy() for i in range(len(preds))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = np.mean(predictions,axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions = predictions.reshape(-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_preds = pd.DataFrame.from_dict({'era':era,'preds':predictions,'target':target.T})\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corrs = df_preds.groupby('era').apply(scoring)\n",
    "corrs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "payout(corrs).mean()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = models_nn[0](data).reshape(-1).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preds = []\n",
    "for model in models_gbm:\n",
    "    preds.append(model.predict(data_))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions_gbm = np.mean(preds,axis=0)\n",
    "df_gbm = pd.DataFrame.from_dict({'era':era,'preds':predictions_gbm,'target':target.T})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_gbm = df_gbm.groupby('era').apply(scoring)\n",
    "corrs_gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payout(corrs_gbm).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['data_type']=='test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wojhed-riDni0-hopnok"
   ]
  }
 ]
}