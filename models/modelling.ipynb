{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from numpy.lib.function_base import _parse_input_dimensions\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, plot_roc_curve\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch._C import dtype\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from pytorch_lightning.core.lightning import LightningModule\n",
        "from pytorch_lightning import Trainer, trainer\n",
        "import datatable as dt\n",
        "import pandas as pd\n",
        "from torch.nn.modules.loss import BCEWithLogitsLoss\n",
        "from group_time_split import GroupTimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, dataloader\n",
        "from torch.utils.data.sampler import BatchSampler, SequentialSampler\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_size, output_size, dims, batch_size, learning_rate=0.05, early_stopping=10,\n",
        "                 model_file='model.pth'):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.dims = dims\n",
        "        self.layer_list = nn.ModuleList()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.loss = nn.BCELoss()\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.early_stopping = early_stopping\n",
        "        self.model_file = model_file\n",
        "        self.batch_size = batch_size\n",
        "        self.train_log = pd.DataFrame({'auc': [0], 'loss': [0]})\n",
        "        self.val_log = pd.DataFrame({'auc': [0], 'loss': [0]})\n",
        "        for i in range(len(self.dims)+1):\n",
        "            if i == 0:\n",
        "                self.layer_list.append(\n",
        "                    nn.Linear(self.input_size, self.dims[i]))\n",
        "                self.layer_list.append(nn.BatchNorm1d(self.dims[i]))\n",
        "            elif i == (len(self.dims)):\n",
        "                self.layer_list.append(\n",
        "                    nn.Linear(self.dims[i-1], self.output_size))\n",
        "            else:\n",
        "                self.layer_list.append(nn.Linear(self.dims[i-1], self.dims[i]))\n",
        "                self.layer_list.append(nn.BatchNorm1d(self.dims[i]))\n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            self.parameters(), lr=self.learning_rate)\n",
        "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer, patience=5, factor=0.1, min_lr=1e-7, eps=1e-08\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.layer_list):\n",
        "            x = F.dropout(F.leaky_relu(self.layer_list[i](x)), p=0.2)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        with torch.set_grad_enabled(True):\n",
        "            x, y = batch['data'].to(\n",
        "                self.device), batch['target'].to(self.device)\n",
        "            x = x.reshape(x.size(1), -1)\n",
        "            y = y.reshape(-1, 1)\n",
        "            logits = self(x)\n",
        "            loss = self.loss(input=logits,\n",
        "                             target=y)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            return {'loss': loss, 'preds': logits, 'target': y}\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        with torch.set_grad_enabled(False):\n",
        "            x, y = batch['data'].to(\n",
        "                self.device), batch['target'].to(self.device)\n",
        "            x = x.reshape(x.size(1), -1)\n",
        "            y = y.reshape(-1, 1)\n",
        "            logits = self(x)\n",
        "            loss = self.loss(logits,\n",
        "                             target=y)\n",
        "            self.scheduler.step(loss)\n",
        "            return {'loss': loss, 'preds': logits, 'target': y}\n",
        "\n",
        "    def eval_step(self, data):\n",
        "        with torch.set_grad_enabled(False):\n",
        "            x, y = data['data'].to(self.device), data['target'].to(self.device)\n",
        "            x = x.reshape(x.size(1), -1)\n",
        "            y = y.reshape(-1, 1)\n",
        "            preds = self(x)\n",
        "            return y, preds\n",
        "\n",
        "    def batch_step_end_metrics(self, num_samples, batch_number, output, running_loss, running_metric):\n",
        "        running_loss += output['loss'].item()\n",
        "        running_metric += roc_auc_score(\n",
        "            output['target'].detach().cpu().numpy(),\n",
        "            output['preds'].detach().cpu().numpy())\n",
        "        return running_loss, running_metric\n",
        "\n",
        "    def epoch_end_metrics(self, outputs):\n",
        "        auc = torch.tensor([roc_auc_score(\n",
        "            out['target'].detach().cpu().numpy(),\n",
        "            out['preds'].detach().cpu().numpy()) for out in outputs])\n",
        "        loss = torch.stack([out['loss'] for out in outputs])\n",
        "        return torch.mean(auc), torch.mean(loss)\n",
        "\n",
        "    def log_results(self, phase, auc, loss):\n",
        "        if phase == 'train':\n",
        "            self.train_log = self.train_log.append(\n",
        "                {'auc': auc.item(), 'loss': loss.item()}, ignore_index=True)\n",
        "        if phase == 'val':\n",
        "            self.val_log = self.val_log.append(\n",
        "                {'auc': auc.item(), 'loss': loss.item()}, ignore_index=True)\n",
        "\n",
        "    def training_loop(self, epochs, dataloaders):\n",
        "        es_counter = 0\n",
        "        auc = {'train': -np.inf, 'eval': -np.inf}\n",
        "        loss = {'train': np.inf, 'eval': np.inf}\n",
        "        best_auc = -np.inf\n",
        "        for e, epoch in enumerate(range(epochs), 1):\n",
        "            for phase in ['train', 'val']:\n",
        "                bar = tqdm(dataloaders[phase])\n",
        "                outs = []\n",
        "                running_loss = 0.0\n",
        "                running_auc = 0.0\n",
        "                for b, batch in enumerate(bar, 1):\n",
        "                    bar.set_description(f'Epoch {epoch} {phase}'.ljust(20))\n",
        "                    if phase == 'train':\n",
        "                        self.train()\n",
        "                        out = self.training_step(batch)\n",
        "                    elif phase == 'val':\n",
        "                        self.eval()\n",
        "                        out = self.validation_step(batch)\n",
        "                    outs.append(out)\n",
        "                    num_samples = batch_size*b\n",
        "                    running_loss, running_auc = self.batch_step_end_metrics(\n",
        "                        num_samples, b, out, running_loss, running_auc)\n",
        "                    bar.set_postfix(loss=f'{running_loss/b:0.5f}',\n",
        "                                    auc=f'{running_auc/b:0.5f}')\n",
        "                auc[phase], loss[phase] = self.epoch_end_metrics(outs)\n",
        "                self.log_results(phase, auc[phase], loss[phase])\n",
        "                if phase == 'val' and auc['val'] > best_auc:\n",
        "                    # print('auc_val: ' + auc['val'], 'best_auc: ' + best_auc)\n",
        "                    best_auc = auc['val']\n",
        "                    best_model_weights = copy.deepcopy(self.state_dict())\n",
        "                    torch.save(best_model_weights, self.model_file)\n",
        "                    es_counter = 0\n",
        "            es_counter += 1\n",
        "            if es_counter > self.early_stopping:\n",
        "                print(\n",
        "                    f'Early Stopping limit reached. Best Model saved to {self.model_file}')\n",
        "                print(f'Best Metric achieved: {best_auc}')\n",
        "                break\n",
        "\n",
        "\n",
        "class FinData(Dataset):\n",
        "    def __init__(self, data, target, mode='train', transform=None, cache_dir=None):\n",
        "        self.data = data\n",
        "        self.target = target\n",
        "        self.mode = mode\n",
        "        self.transform = transform\n",
        "        self.cache_dir = cache_dir\n",
        "        self.date = date\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index.to_list()\n",
        "        if self.transform:\n",
        "            return self.transform(self.data.iloc[index].values)\n",
        "        else:\n",
        "            if type(index) is list:\n",
        "                sample = {\n",
        "                    'target': torch.Tensor(self.target.iloc[index].values),\n",
        "                    'data': torch.FloatTensor(self.data[index]),\n",
        "                    'date': torch.Tensor(self.date.iloc[index].values)\n",
        "                }\n",
        "                return sample\n",
        "            else:\n",
        "                sample = {\n",
        "                    'target': torch.Tensor(self.target.iloc[index]),\n",
        "                    'data': torch.FloatTensor(self.data[index]),\n",
        "                    'date': torch.Tensor(self.date.iloc[index])\n",
        "                }\n",
        "                return sample\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        nn.init.xavier_normal_(m.weight, nn.init.calculate_gain('leaky_relu'))\n",
        "        m.bias.data.fill_(1)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "def load_data(root_dir, mode, overide=None):\n",
        "    if overide:\n",
        "        data = dt.fread(overide).to_pandas()\n",
        "    elif mode == 'train':\n",
        "        data = dt.fread(root_dir+'train.csv').to_pandas()\n",
        "    elif mode == 'test':\n",
        "        data = dt.fread(root_dir+'example_test.csv').to_pandas()\n",
        "    elif mode == 'sub':\n",
        "        data = dt.fread(root_dir+'example_sample_submission.csv').to_pandas()\n",
        "    return data\n",
        "\n",
        "\n",
        "def preprocess_data(data):\n",
        "    # data = data.query('weight > 0').reset_index(drop=True)\n",
        "    data['action'] = ((data['resp'].values) > 0).astype('float32')\n",
        "    features = [\n",
        "        col for col in data.columns if 'feature' in col and col != 'feature_0']+['weight']\n",
        "    for col in features:\n",
        "        data[col].fillna(data[col].mean(), inplace=True)\n",
        "    target = data['action']\n",
        "    date = data['date']\n",
        "    data = data[features]\n",
        "    scaler = StandardScaler()\n",
        "    data = scaler.fit_transform(data)\n",
        "\n",
        "    return data, target, features, date\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data = load_data('data/', mode='train', overide='filtered_train.csv')\n",
        "data, target, features, date = preprocess_data(data)\n",
        "# %%\n",
        "dataset = FinData(data, target, features)\n",
        "# %%\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "dims = [384, 896, 896, 394]\n",
        "batch_size = 1000\n",
        "epochs = 100\n",
        "gts = GroupTimeSeriesSplit()\n",
        "train_model = False\n",
        "eval_model = True\n",
        "for i, (train_idx, val_idx) in enumerate(gts.split(data, groups=date)):\n",
        "    if train_model:\n",
        "        model = Classifier(input_size=len(features), output_size=1,\n",
        "                           dims=dims, batch_size=batch_size,\n",
        "                           model_file=f'nn_model_fold_{i}.pth').to(device=device)\n",
        "\n",
        "        # model.apply(init_weights)\n",
        "        train_set, val_set = Subset(\n",
        "            dataset, train_idx), Subset(dataset, val_idx)\n",
        "        train_sampler = BatchSampler(SequentialSampler(\n",
        "            train_set), batch_size=batch_size, drop_last=False)\n",
        "        val_sampler = BatchSampler(SequentialSampler(\n",
        "            val_set), batch_size=batch_size, drop_last=False)\n",
        "        dataloaders = {'train': DataLoader(dataset, sampler=train_sampler, num_workers=6),\n",
        "                       'val': DataLoader(dataset, sampler=val_sampler, num_workers=6)}\n",
        "        model.training_loop(epochs=epochs, dataloaders=dataloaders)\n",
        "        model.train_log.to_csv(\n",
        "            f'logs/train_fold_{i}_{str(datetime.datetime.now())}.csv')\n",
        "        model.val_log.to_csv(\n",
        "            f'logs/val_fold_{i}_{str(datetime.datetime.now())}.csv')\n",
        "    if eval_model:\n",
        "        model = Classifier(input_size=len(features), output_size=1,\n",
        "                           dims=dims, batch_size=batch_size,\n",
        "                           model_file=f'nn_model_fold_{i}.pth').to(device=device)\n",
        "        checkpoint = torch.load(model.model_file)\n",
        "        model.load_state_dict(checkpoint)\n",
        "        model.eval()\n",
        "        val_set = Subset(dataset, val_idx)\n",
        "        val_sampler = BatchSampler(SequentialSampler(\n",
        "            val_set), batch_size=batch_size, drop_last=False)\n",
        "        val_loader = DataLoader(dataset, sampler=val_sampler, num_workers=6)\n",
        "        bar = tqdm(val_loader)\n",
        "        all_preds = []\n",
        "        all_y_true = []\n",
        "        for b, batch in enumerate(bar, 1):\n",
        "            bar.set_description(f'Evaluating Model')\n",
        "            y_true, preds = model.eval_step(batch)\n",
        "            all_y_true.append(y_true.cpu().numpy())\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "        all_preds = np.concatenate(all_preds, axis=0)\n",
        "        all_y_true = np.concatenate(all_y_true, axis=0)\n",
        "        fpr, tpr, _ = roc_curve(all_y_true, all_preds)\n",
        "        plt.plot(fpr, tpr, label='nn')\n",
        "        plt.savefig(\n",
        "            f'plots/val_fold_{i}_roc_curve.png')\n",
        "# %%\n",
        "val_set[:]['target']\n",
        "# %%\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}