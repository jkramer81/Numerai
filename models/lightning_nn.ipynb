{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import copy\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from numba import njit\n",
        "from pytorch_lightning import Callback\n",
        "from pytorch_lightning import loggers as pl_loggers\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from torch.nn.modules import linear\n",
        "from torch.nn.modules.batchnorm import BatchNorm1d\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "class MetricsCallback(Callback):\n",
        "    \"\"\"PyTorch Lightning metric callback.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.metrics = []\n",
        "\n",
        "    def on_validation_end(self, trainer, pl_module):\n",
        "        self.metrics.append(trainer.callback_metrics)\n",
        "\n",
        "\n",
        "class Classifier(pl.LightningModule):\n",
        "    def __init__(self, input_size, output_size, params=None,\n",
        "                 model_path='models/'):\n",
        "        super(Classifier, self).__init__()\n",
        "        dim_1 = params['dim_1']\n",
        "        dim_2 = params['dim_2']\n",
        "        dim_3 = params['dim_3']\n",
        "        dim_4 = params['dim_4']\n",
        "        dim_5 = params['dim_5']\n",
        "        self.dropout_prob = params['dropout']\n",
        "        self.lr = params['lr']\n",
        "        self.activation = params['activation']\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.loss = params['loss']\n",
        "        self.weight_decay = params['weight_decay']\n",
        "        self.amsgrad = params['amsgrad']\n",
        "        self.label_smoothing = params['label_smoothing']\n",
        "        self.train_log = pd.DataFrame({'auc': [0], 'loss': [0]})\n",
        "        self.val_log = pd.DataFrame({'auc': [0], 'loss': [0]})\n",
        "        self.model_path = model_path\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.BatchNorm1d(input_size),\n",
        "            nn.Linear(input_size, dim_1, bias=False),\n",
        "            nn.BatchNorm1d(dim_1),\n",
        "            self.activation(),\n",
        "            nn.Dropout(p=self.dropout_prob),\n",
        "            nn.Linear(dim_1, dim_2, bias=False),\n",
        "            nn.BatchNorm1d(dim_2),\n",
        "            self.activation(),\n",
        "            nn.Dropout(p=self.dropout_prob),\n",
        "            nn.Linear(dim_2, dim_3, bias=False),\n",
        "            nn.BatchNorm1d(dim_3),\n",
        "            self.activation(),\n",
        "            nn.Dropout(p=self.dropout_prob),\n",
        "            nn.Linear(dim_3, dim_4, bias=False),\n",
        "            nn.BatchNorm1d(dim_4),\n",
        "            self.activation(),\n",
        "            nn.Dropout(p=self.dropout_prob),\n",
        "            nn.Linear(dim_4, dim_5, bias=False),\n",
        "            nn.BatchNorm1d(dim_5),\n",
        "            self.activation(),\n",
        "            nn.Dropout(p=self.dropout_prob),\n",
        "            nn.Linear(dim_5, self.output_size, bias=False)\n",
        "        )\n",
        "        self.encoder_1l = nn.Sequential(\n",
        "            nn.BatchNorm1d(input_size),\n",
        "            nn.Linear(input_size, dim_1, bias=False),\n",
        "            nn.BatchNorm1d(dim_1),\n",
        "            self.activation(),\n",
        "            nn.Dropout(p=self.dropout_prob),\n",
        "            nn.Linear(dim_1, self.output_size, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.encoder_1l(x)\n",
        "        return out\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch['data'], batch['target']\n",
        "        x = x.view(x.size(1), -1)\n",
        "        y = y.T\n",
        "        logits = self(x)\n",
        "        loss = self.loss(input=logits, target=y)\n",
        "        mse = mean_squared_error(y_true=y.cpu().numpy(),\n",
        "                                 y_pred=logits.cpu().detach().numpy())\n",
        "        self.log('train_mse', mse, on_step=False,\n",
        "                 on_epoch=True, prog_bar=True)\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        return {'loss': loss}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch['data'], batch['target']\n",
        "        x = x.view(x.size(1), -1)\n",
        "        y = y.T\n",
        "        logits = self(x)\n",
        "        loss = self.loss(input=logits,\n",
        "                         target=y)\n",
        "        mse = mean_squared_error(y_true=y.cpu().numpy(),\n",
        "                                 y_pred=logits.cpu().detach().numpy())\n",
        "        return {'loss': loss, 'y': y, 'logits': logits, 'mse': mse}\n",
        "\n",
        "    def validation_epoch_end(self, val_step_outputs):\n",
        "        epoch_loss = torch.tensor([x['loss'] for x in val_step_outputs]).mean()\n",
        "        epoch_mse = torch.tensor([x['mse'] for x in val_step_outputs]).mean()\n",
        "        self.log('val_loss', epoch_loss, prog_bar=True)\n",
        "        self.log('val_mse', epoch_mse, prog_bar=True)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        return self.validation_step(batch, batch_idx)\n",
        "\n",
        "    def test_epoch_end(self, outputs):\n",
        "        epoch_loss = torch.tensor([x['loss'] for x in outputs]).mean()\n",
        "        epoch_auc = torch.tensor([x['auc'] for x in outputs]).mean()\n",
        "        self.log('test_loss', epoch_loss)\n",
        "        self.log('test_auc', epoch_auc)\n",
        "\n",
        "    def predict(self, batch):\n",
        "        self.eval()\n",
        "        x, y = batch['data'], batch['target']\n",
        "        x = x.view(x.size(1), -1)\n",
        "        x = self(x)\n",
        "        return torch.sigmoid(x.view(-1))\n",
        "\n",
        "    def prediction_loop(self, dataloader, return_tensor=True):\n",
        "        bar = tqdm(dataloader)\n",
        "        preds = []\n",
        "        for batch in bar:\n",
        "            preds.append(self.predict(batch))\n",
        "        if return_tensor:\n",
        "            return torch.cat(preds, dim=0)\n",
        "        else:\n",
        "            return preds\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # weight_decay = self.weight_decay,\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr,\n",
        "                                     amsgrad=self.amsgrad)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer, patience=5, factor=0.1, min_lr=1e-7, eps=1e-08\n",
        "        )\n",
        "        return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'val_mse'}\n",
        "\n",
        "\n",
        "def init_weights(m, func):\n",
        "    if type(m) == nn.Linear:\n",
        "        nn.init.xavier_normal_(m.weight, nn.init.calculate_gain(func))\n",
        "        # m.bias.data.fill_(1)\n",
        "\n",
        "\n",
        "def train_cross_val(p):\n",
        "    data_ = load_data(root_dir='./data/', mode='train')\n",
        "    data_, target_, features, date = preprocess_data(data_, nn=True)\n",
        "\n",
        "    gts = PurgedGroupTimeSeriesSplit(n_splits=5, group_gap=5)\n",
        "\n",
        "    input_size = data_.shape[-1]\n",
        "    output_size = 1\n",
        "    tb_logger = pl_loggers.TensorBoardLogger('logs/')\n",
        "    models = []\n",
        "    for i, (train_idx, val_idx) in enumerate(gts.split(data_, groups=date)):\n",
        "        idx = np.concatenate([train_idx, val_idx])\n",
        "        data = copy.deepcopy(data_[idx])\n",
        "        target = copy.deepcopy(target_[idx])\n",
        "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "            os.path.join('models/', \"fold_{}\".format(i)), monitor=\"val_auc\", mode='max', save_top_k=1, period=10)\n",
        "        model = Classifier(input_size=input_size,\n",
        "                           output_size=output_size, params=p)\n",
        "        if p['activation'] == nn.ReLU:\n",
        "            model.apply(lambda m: init_weights(m, 'relu'))\n",
        "        elif p['activation'] == nn.LeakyReLU:\n",
        "            model.apply(lambda m: init_weights(m, 'leaky_relu'))\n",
        "        train_idx = [i for i in range(0, max(train_idx) + 1)]\n",
        "        val_idx = [i for i in range(len(train_idx), len(idx))]\n",
        "        data[train_idx] = calc_data_mean(\n",
        "            data[train_idx], './cache', train=True, mode='mean')\n",
        "        data[val_idx] = calc_data_mean(\n",
        "            data[val_idx], './cache', train=False, mode='mean')\n",
        "        dataset = FinData(data=data, target=target, date=date)\n",
        "        dataloaders = create_dataloaders(\n",
        "            dataset, indexes={'train': train_idx, 'val': val_idx}, batch_size=p['batch_size'])\n",
        "        es = EarlyStopping(monitor='val_auc', patience=10,\n",
        "                           min_delta=0.0005, mode='max')\n",
        "        trainer = pl.Trainer(logger=tb_logger,\n",
        "                             max_epochs=500,\n",
        "                             gpus=1,\n",
        "                             callbacks=[checkpoint_callback, es],\n",
        "                             precision=16)\n",
        "        trainer.fit(\n",
        "            model, train_dataloader=dataloaders['train'], val_dataloaders=dataloaders['val'])\n",
        "        torch.save(model.state_dict(), f'models/fold_{i}_state_dict.pth')\n",
        "        models.append(model)\n",
        "    return models, features\n",
        "\n",
        "\n",
        "def final_train(p, load=False):\n",
        "    data_ = load_data(root_dir='./data/', mode='train')\n",
        "    data, target, features, date = preprocess_data(data_, nn=True)\n",
        "    input_size = data.shape[-1]\n",
        "    output_size = 1\n",
        "    train_idx, val_idx = date[date <= 450].index.values.tolist(\n",
        "    ), date[date > 450].index.values.tolist()\n",
        "    data[train_idx] = calc_data_mean(data[train_idx], './cache', train=True)\n",
        "    data[val_idx] = calc_data_mean(data[val_idx], './cache', train=False)\n",
        "    checkpoint_callback = pl.callbacks.ModelCheckpoint(filepath='models/full_train',\n",
        "                                                       monitor=\"val_auc\", mode='max', save_top_k=1, period=10)\n",
        "    model = Classifier(input_size=input_size,\n",
        "                       output_size=output_size, params=p)\n",
        "    if p['activation'] == nn.ReLU:\n",
        "        model.apply(lambda m: init_weights(m, 'relu'))\n",
        "    elif p['activation'] == nn.LeakyReLU:\n",
        "        model.apply(lambda m: init_weights(m, 'leaky_relu'))\n",
        "    dataset = FinData(data, target, date)\n",
        "    dataloaders = create_dataloaders(\n",
        "        dataset, indexes={'train': train_idx, 'val': val_idx}, batch_size=p['batch_size'])\n",
        "    es = EarlyStopping(monitor='val_auc', patience=10,\n",
        "                       min_delta=0.0005, mode='max')\n",
        "    trainer = pl.Trainer(max_epochs=500,\n",
        "                         gpus=1,\n",
        "                         callbacks=[checkpoint_callback, es],\n",
        "                         precision=16)\n",
        "    trainer.fit(\n",
        "        model, train_dataloader=dataloaders['train'], val_dataloaders=dataloaders['val'])\n",
        "    torch.save(model.state_dict(), 'models/final_train.pth')\n",
        "    return model, features\n",
        "\n",
        "\n",
        "def fillna_npwhere(array, values):\n",
        "    if np.isnan(array.sum()):\n",
        "        array = np.nan_to_num(array) + np.isnan(array) * values\n",
        "    return array\n",
        "\n",
        "\n",
        "def test_model(models, features, cache_dir='cache'):\n",
        "    env = janestreet.make_env()\n",
        "    iter_test = env.iter_test()\n",
        "    if type(models) == list:\n",
        "        models = [model.eval() for model in models]\n",
        "    else:\n",
        "        models.eval()\n",
        "    f_mean = np.load(f'{cache_dir}/f_mean.npy')\n",
        "    for (test_df, sample_prediction_df) in tqdm(iter_test):\n",
        "        if test_df['weight'].item() > 0:\n",
        "            vals = torch.FloatTensor(\n",
        "                fillna_npwhere(test_df[features].values, f_mean))\n",
        "            if type(models) == list:\n",
        "                preds = [torch.sigmoid(model.forward(vals.view(1, -1))).item()\n",
        "                         for model in models]\n",
        "                pred = np.median(preds)\n",
        "            else:\n",
        "                pred = torch.sigmoid(models.forward(vals.view(1, -1))).item()\n",
        "            sample_prediction_df.action = np.where(\n",
        "                pred > 0.5, 1, 0).astype(int).item()\n",
        "        else:\n",
        "            sample_prediction_df.action = 0\n",
        "        env.predict(sample_prediction_df)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "def main(train=True):\n",
        "    p = {'batch_size':   4986, 'dim_1': 248, 'dim_2': 487,\n",
        "         'dim_3':        269, 'dim_4': 218, 'dim_5': 113,\n",
        "         'activation':   nn.ReLU, 'dropout': 0.01563457578202565,\n",
        "         'lr':           0.00026372556533974916, 'label_smoothing': 0.06834918091900156,\n",
        "         'weight_decay': 0.005270589494631074, 'amsgrad': False}\n",
        "    if train:\n",
        "        models, features = train_cross_val(p)\n",
        "        # models, features = final_train(p, load=False)\n",
        "    else:\n",
        "        data_ = load_data(root_dir='./data/', mode='train')\n",
        "        data_, target_, features, date = preprocess_data(data_, nn=True)\n",
        "        model_path = '/kaggle/input/model-files'\n",
        "        f_mean = calc_data_mean(data_, 'cache')\n",
        "        models = load_model(model_path, data_.shape[-1], 1, p, False)\n",
        "    # model, checkpoint = final_train(p)\n",
        "    # best_model_path = checkpoint.best_model_path\n",
        "    # model, features = final_train(load=best_model_path)\n",
        "    test_model(models, features)\n",
        "    return models\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = main()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}